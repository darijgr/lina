\documentclass[numbers=enddot,12pt,final,onecolumn,notitlepage]{scrartcl}%
\usepackage[headsepline,footsepline,manualmark]{scrlayer-scrpage}
\usepackage[all,cmtip]{xy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{comment}
\usepackage{color}
\usepackage{hyperref}
\usepackage{datetime}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{needspace}
\usepackage{tabls}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Saturday, September 17, 2016 02:09:37}
%TCIDATA{SuppressPackageManagement}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Language=American English}
%TCIDATA{ComputeDefs=
%$B=\left(
%\begin{array}
%[c]{cc}%
%1 & 1\\
%1 & 0
%\end{array}
%\right)  $
%}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\newcounter{exer}
\newcounter{exera}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[section]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{condition}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{quest}[theo]{Question}
\newenvironment{algorithm}[1][]
{\begin{quest}[#1]\begin{leftbar}}
{\end{leftbar}\end{quest}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{conclusion}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{conj}[theo]{Conjecture}
\newenvironment{conjecture}[1][]
{\begin{conj}[#1]\begin{leftbar}}
{\end{leftbar}\end{conj}}
\newtheorem{exam}[theo]{Example}
\newenvironment{example}[1][]
{\begin{exam}[#1]\begin{leftbar}}
{\end{leftbar}\end{exam}}
\newtheorem{exmp}[exer]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exmp}[#1]\begin{leftbar}}
{\end{leftbar}\end{exmp}}
\newenvironment{statement}{\begin{quote}}{\end{quote}}
\iffalse
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\fi
\let\sumnonlimits\sum
\let\prodnonlimits\prod
\let\cupnonlimits\bigcup
\let\capnonlimits\bigcap
\renewcommand{\sum}{\sumnonlimits\limits}
\renewcommand{\prod}{\prodnonlimits\limits}
\renewcommand{\bigcup}{\cupnonlimits\limits}
\renewcommand{\bigcap}{\capnonlimits\limits}
\setlength\tablinesep{3pt}
\setlength\arraylinesep{3pt}
\setlength\extrarulesep{3pt}
\voffset=0cm
\hoffset=-0.7cm
\setlength\textheight{22.5cm}
\setlength\textwidth{15.5cm}
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\newcommand{\id}{\operatorname{id}}
\ihead{Notes on linear algebra (\today, \currenttime)}
\ohead{page \thepage}
\cfoot{}
\begin{document}

\title{Notes on linear algebra}
\author{Darij Grinberg}
\date{
%TCIMACRO{\TeXButton{today}{\today}}%
%BeginExpansion
\today
%EndExpansion
}
\maketitle
\tableofcontents

\section{Preface}

These notes are accompanying \href{http://www.math.umn.edu/~dgrinber/4242/}{a
class on applied linear algebra (Math 4242) I am giving at the University of
Minneapolis in Fall 2016} (the website of the class is
\url{http://www.math.umn.edu/~dgrinber/4242/} ). They contain both the
material of the class (although with no promise of timeliness!) and the
homework exercises (and possibly some additional exercises).

There will (probably) be no actual applications in these notes, but only the
mathematical material used in these applications. If time allows, the notes
will contain tutorials on the use of \href{http://www.sagemath.org/}{SageMath}
(a computer algebra system suited both for numerical and for algebraic computations).

Sections marked with an asterisk (*) are not a required part of the Math 4242 course.

Several good books have been written on linear algebra; these notes are not
supposed to replace any of them. Let me just mention four sources I can
recommend\footnote{I have \textbf{not} read any of the books myself (apart
from fragments). My recommendations are based on cursory skimming and random
appraisal of specific points; do not mistake my recommendations for
guarantees.}:

\begin{itemize}
\item Olver's and Shakiban's \cite{OlvSha06} is the traditional text for Math
4242 at UMN. It might be the best place to learn about the applications of
linear algebra.

\item Hefferon's \cite{Heffer16} is a free text that does things slowly but
rigorously (at least for the standards of an introductory linear-algebra
text). It has plenty of examples (and exercises with solutions), fairly
detailed proofs, and occasional applications. (Which is why it is over 500
pages long; I hope you can easily decide what to skip based on your
preferences.) Altogether, I think it does many things very well. The main
drawback is its lack of the theory of bilinear forms (but I don't know if we
will even have time for that).

\item Lankham's, Nachtergaele's and Schilling's \cite{LaNaSc16} is a set of
notes for introductory linear algebra, doing the abstract side (vector spaces,
linear maps) early on and in some detail.

\item Treil's \cite{Treil15} is another free text; this is written for a more
mathematically mature reader, and has a slight bias towards the linear algebra
useful for functional analysis.\footnote{The title of the book is a play on
Axler's \textquotedblleft Linear Algebra Done Right\textquotedblright, which
is biased towards analysis (or, rather, against algebra) to a ridiculous
extent. Axler seems to write really well, but the usefulness of this book is
severely limited by its obstinate avoidance of anything that looks too
explicit and algebraic.}
\end{itemize}

{\small [Please let the authors know if you find any errors or unclarities.
Feel free to ask me if you want your doubts resolved beforehand.]}

Also, some previous iterations of Math 4242 have left behind interesting notes:

\begin{itemize}
\item Stephen Lewis, Fall 2014, \url{http://www.stephen-lewis.net/4242/}
(enable javascript!).

\item Nathalie Sheils, Fall 2015,
\url{http://math.umn.edu/~nesheils/F15_M4242/LectureNotes.html} (yes, those
are on dropbox).
\end{itemize}

There are countless other sets of lecture notes on the internet, books in the
library, and even books on the internet if you know where to look. You can
find an overview of (published, paper) books in \cite{Drucker12} (but usually
without assessing their quality), and another (with reviews) on the MAA
website \url{http://www.maa.org/tags/linear-algebra} . (Reviews on Amazon and
goodreads are usually just good for a laugh.)

The notes you are reading are under construction, and will remain so for at
least the whole Fall term 2016. Please let me know of any errors and
unclarities you encounter (my email address is \texttt{dgrinber@umn.edu}%
)\footnote{The sourcecode of the notes is also publicly available at
\url{https://github.com/darijgr/lina} .}. Thank you!

\subsection{Acknowledgments}

I would like to thank Mark Richard for correcting a typo in the notes.

\section{\label{chp.intro}Introduction to matrices}

\subsection{Matrices and entries}

In the following, we shall study matrices filled with numbers. This is not the
most general thing to study (we could also fill matrices with other things,
such as polynomials -- and in fact, such matrices are highly useful); nor will
we be very precise about it. In fact, for the first few sections of this
chapter, we shall not even specify what we mean by \textquotedblleft
numbers\textquotedblright, even though the word \textquotedblleft
number\textquotedblright\ is far from being a well-defined notion. However, as
soon as we start caring about (say) computer calculations, we will have to
spend some words specifying our \textquotedblleft numbers\textquotedblright%
\ more precisely.

We shall use the symbol $\mathbb{N}$ for the set $\left\{  0,1,2,\ldots
\right\}  $. This is the set of all nonnegative integers.

\begin{definition}
If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, then an $n\times m$\textit{-matrix}
simply means a rectangular table with $n$ rows and $m$ columns, such that each
cell is filled with a number.
\end{definition}

For example, $\left(
\begin{array}
[c]{ccc}%
1 & 7 & 2\\
-\sqrt{2} & 6 & 1/3
\end{array}
\right)  $ is a $2\times3$-matrix, whereas $\left(
\begin{array}
[c]{c}%
1\\
-2\\
0
\end{array}
\right)  $ is a $3\times1$-matrix.\footnote{For the friends of stupid examples
(me?), here are two more: $\left(
\vphantom{\begin{array}{c} a \\ a \\ a \end{array}}\right)  $ is a $3\times
0$-matrix (it contains no cells, and thus no numbers), and $\left(
\phantom{\begin{array}{ccc} a & a & a \end{array}}\right)  $ is a $0\times
3$-matrix (again, with no numbers because it has no cells). For various
technical reasons (\cite{deBoor}), it is helpful to regard such empty matrices
as different.}

\begin{definition}
The word \textquotedblleft\textit{matrix}\textquotedblright\ will encompass
all $n\times m$-matrices for all possible values of $n$ and $m$.
\end{definition}

\begin{definition}
The \textit{dimensions} of an $n\times m$-matrix $A$ are the two integers $n$
and $m$. When they are equal (that is, $n=m$), we say that $A$ is a
\textit{square matrix}, and call $n$ its \textit{size}.
\end{definition}

\begin{definition}
If $A$ is an $n\times m$-matrix, and if $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,m\right\}  $, then $A_{i,j}$ will denote the
entry of $A$ in row $i$ and column $j$. This entry is also called the $\left(
i,j\right)  $\textit{-th entry of }$A$ (or simply the $\left(  i,j\right)
$\textit{-entry of }$A$).
\end{definition}

For example, $\left(
\begin{array}
[c]{ccc}%
1 & 7 & 2\\
-\sqrt{2} & 6 & 1/3
\end{array}
\right)  _{1,3}=2$. Note that this notation $A_{i,j}$ (for the $\left(
i,j\right)  $-th entry of a matrix $A$) is not standard in literature! Some
authors instead use $A_{j}^{i}$ (the $i$ on top is not an exponent, but just a
superscript) or $a_{i,j}$ (where $a$ is the lowercase letter corresponding to
the uppercase letter $A$ denoting the matrix\footnote{This notation is bad for
two reasons: First, it forces you to always denote matrices by uppercase
letters; second, it doesn't let you write things like $\left(
\begin{array}
[c]{ccc}%
1 & 7 & 2\\
-\sqrt{2} & 6 & 1/3
\end{array}
\right)  _{1,3}$.}). Many authors often drop the comma between $i$ and $j$ (so
they call it $A_{ij}$ or $a_{ij}$); this notation is slightly ambiguous (does
$A_{132}$ mean $A_{13,2}$ or $A_{1,32}$ ?). Unfortunately, some authors use
the notation $A_{i,j}$ for something else called a \textit{cofactor} of $A$
(which is, in a sense, quite the opposite of the $\left(  i,j\right)  $-th
entry of $A$); but we will never do this here (and probably we will not really
get into cofactors anyway).

\subsection{The matrix builder notation}

I would like to do something interesting, but I am forced to introduce more
notations. Please have patience with me. Let me introduce a notation for
building a matrix out of a bunch of entries:

\begin{definition}
Let $n\in\mathbb{N}$. Assume that you are given a number $a_{i,j}$ for each
pair $\left(  i,j\right)  $ of an integer $i\in\left\{  1,2,\ldots,n\right\}
$ and $j\in\left\{  1,2,\ldots,m\right\}  $. Then, $\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$ shall denote the $n\times m$-matrix whose
$\left(  i,j\right)  $-th entry is $a_{i,j}$ for all $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}  $. (To say it
differently: $\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ shall
denote the $n\times m$-matrix $A$ such that $A_{i,j}=a_{i,j}$ for all
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}
$. In other words,%
\[
\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,m}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,m}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,m}%
\end{array}
\right)  .
\]
)
\end{definition}

Some examples:

\begin{itemize}
\item We have $\left(  i-j\right)  _{1\leq i\leq2,\ 1\leq j\leq3}=\left(
\begin{array}
[c]{ccc}%
1-1 & 1-2 & 1-3\\
2-1 & 2-2 & 2-3
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
0 & -1 & -2\\
1 & 0 & -1
\end{array}
\right)  $.

\item We have $\left(  i+j\right)  _{1\leq i\leq3,\ 1\leq j\leq2}=\left(
\begin{array}
[c]{cc}%
1+1 & 1+2\\
2+1 & 2+2\\
3+1 & 3+2
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
2 & 3\\
3 & 4\\
4 & 5
\end{array}
\right)  $.

\item We have $\left(  \dfrac{i+1}{j}\right)  _{1\leq i\leq3,\ 1\leq j\leq
3}=\left(
\begin{array}
[c]{ccc}%
\dfrac{1+1}{1} & \dfrac{1+1}{2} & \dfrac{1+1}{3}\\
\dfrac{2+1}{1} & \dfrac{2+1}{2} & \dfrac{2+1}{3}\\
\dfrac{3+1}{1} & \dfrac{3+1}{2} & \dfrac{3+1}{3}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
2 & 1 & \dfrac{2}{3}\\
3 & \dfrac{3}{2} & 1\\
4 & 2 & \dfrac{4}{3}%
\end{array}
\right)  $.

\item We have $\left(  \dfrac{i-j}{i+j}\right)  _{1\leq i\leq3,\ 1\leq j\leq
2}=\left(
\begin{array}
[c]{cc}%
\dfrac{1-1}{1+1} & \dfrac{1-2}{1+2}\\
\dfrac{2-1}{2+1} & \dfrac{2-2}{2+2}\\
\dfrac{3-1}{3+1} & \dfrac{3-2}{3+2}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
0 & -\dfrac{1}{3}\\
\dfrac{1}{3} & 0\\
\dfrac{1}{2} & \dfrac{1}{5}%
\end{array}
\right)  $.
\end{itemize}

The notation $\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ is
fairly standard (you will be understood if you use it), though again there are
variations in the literature.

We used the two letters $i$ and $j$ in the notation $\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$, but we could just as well have picked any
other two letters (as long as they aren't already taken for something else).
For example, $\left(  xy\right)  _{1\leq x\leq2,\ 1\leq y\leq2}=\left(
\begin{array}
[c]{cc}%
1\cdot1 & 1\cdot2\\
2\cdot1 & 2\cdot2
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
1 & 2\\
2 & 4
\end{array}
\right)  $.

Of course, if you decompose an $n\times m$-matrix $A$ into its entries, and
then assemble these entries back into an $n\times m$-matrix (arranged in the
same way as in $A$), then you get back $A$. In other words: For every $n\times
m$-matrix $A$, we have%
\begin{equation}
\left(  A_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}=A.
\label{eq.matrix.A-through-Aij}%
\end{equation}


\subsection{Row and column vectors}

Here is some more terminology:

\begin{definition}
Let $n\in\mathbb{N}$. A \textit{row vector of size }$n$ means a $1\times
n$-matrix. A \textit{column vector of size }$n$ means an $n\times1$-matrix.
\end{definition}

For example, $\left(
\begin{array}
[c]{cc}%
a & b
\end{array}
\right)  $ is a row vector of size $2$, while $\left(
\begin{array}
[c]{c}%
a\\
b
\end{array}
\right)  $ is a column vector of size $2$.

The following definition is common-sense:

\begin{definition}
Let $n\in\mathbb{N}$. If $v$ is a row vector of size $n$, then the $\left(
1,j\right)  $-th entry of $v$ (for $j\in\left\{  1,2,\ldots,n\right\}  $) will
also be called the $j$\textit{-th entry} of $v$ (because $v$ has only one row,
so that we don't have to say which row an entry lies in). If $v$ is a column
vector of size $n$, then the $\left(  i,1\right)  $-th entry of $v$ (for
$i\in\left\{  1,2,\ldots,n\right\}  $) will also be called the $i$\textit{-th
entry} of $v$.
\end{definition}

\subsection{Transposes}

\begin{definition}
The transpose of an $n\times m$-matrix $A$ is defined to be the $m\times
n$-matrix $\left(  A_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$. It is
denoted by $A^{T}$.
\end{definition}

Let us unravel this confusing-looking definition! It says that the transpose
of an $n\times m$-matrix $A$ is the $m\times n$-matrix whose $\left(
i,j\right)  $-th entry (for $i\in\left\{  1,2,\ldots,m\right\}  $ and
$j\in\left\{  1,2,\ldots,n\right\}  $) is the $\left(  j,i\right)  $-th entry
of $A$. So the transpose of $A$ has the very same entries as $A$, but in
different position: namely, the entry in position $\left(  i,j\right)  $ gets
moved into position $\left(  j,i\right)  $. In other words, the entry that was
in row $i$ and column $j$ gets moved into column $i$ and row $j$. So, visually
speaking, the transpose of the matrix $A$ is obtained by \textquotedblleft
reflecting $A$ around the diagonal\textquotedblright. Some examples should
help clarify this:%
\begin{align*}
\left(
\begin{array}
[c]{ccc}%
a & b & c\\
a^{\prime} & b^{\prime} & c^{\prime}%
\end{array}
\right)  ^{T}  &  =\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
b & b^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  ;\\
\left(
\begin{array}
[c]{cc}%
a & b\\
a^{\prime} & b^{\prime}%
\end{array}
\right)  ^{T}  &  =\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
b & b^{\prime}%
\end{array}
\right)  ;\\
\left(
\begin{array}
[c]{c}%
a\\
b\\
c
\end{array}
\right)  ^{T}  &  =\left(
\begin{array}
[c]{ccc}%
a & b & c
\end{array}
\right)  .
\end{align*}


Transposes have many uses, but for now we stress one particular use: as a
space-saving device. Namely, if you work with column vectors, you quickly
notice that they take up a lot of vertical space in writing: just see by how
much the column vector $\left(
\begin{array}
[c]{c}%
4\\
-1\\
2\\
0
\end{array}
\right)  $ has stretched the spacing between its line and the lines above and
below\footnote{Additionally, column vectors of size $2$ have the annoying
property that they can get confused for binomial coefficients. To wit,
$\left(
\begin{array}
[c]{c}%
4\\
2
\end{array}
\right)  $ denotes a column vector, whereas $\dbinom{4}{2}$ denotes a binomial
coefficient (which equals the number $6$). The only way to tell them apart is
by the amount of empty space between the parentheses and the entries; this is
not a very reliable way to keep different notations apart.}! It is much more
economical to rewrite it as the transpose of a row vector: $\left(
\begin{array}
[c]{cccc}%
4 & -1 & 2 & 0
\end{array}
\right)  ^{T}$. It is furthermore common to write row vectors as tuples (i.e.,
put commas between their entries instead of leaving empty space); thus, the
row vector $\left(
\begin{array}
[c]{cccc}%
4 & -1 & 2 & 0
\end{array}
\right)  $ becomes $\left(  4,-1,2,0\right)  $ (which takes up less space),
and our column vector above becomes $\left(  4,-1,2,0\right)  ^{T}$.

The transpose of a matrix $A$ is also denoted by $A^{t}$ or $^{T}A$ or $^{t}A$
by various authors (not me).

Here is a very simple fact about transposes: The transpose of the transpose of
a matrix $A$ is the matrix $A$ itself. In other words:

\begin{proposition}
\label{prop.transpose.invol}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times m$-matrix. Then, $\left(  A^{T}\right)  ^{T}=A$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.transpose.invol}.]This is fairly clear, but
let me give a formal proof just to get you used to the notations.

We have $A^{T}=\left(  A_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$ (by
the definition of $A^{T}$). Thus, $A^{T}$ is an $m\times n$-matrix and
satisfies%
\begin{equation}
\left(  A^{T}\right)  _{i,j}=A_{j,i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,\ldots,m\right\}  \text{ and }j\in\left\{  1,2,\ldots
,n\right\}  . \label{pf.prop.transpose.invol.AT}%
\end{equation}
Hence,%
\begin{equation}
\left(  A^{T}\right)  _{j,i}=A_{i,j}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,\ldots,n\right\}  \text{ and }j\in\left\{  1,2,\ldots
,m\right\}  . \label{pf.prop.transpose.invol.AT2}%
\end{equation}
(indeed, this follows by applying (\ref{pf.prop.transpose.invol.AT}) to $j$
and $i$ instead of $i$ and $j$).

Now, the definition of $\left(  A^{T}\right)  ^{T}$ yields%
\[
\left(  A^{T}\right)  ^{T}=\left(  \underbrace{\left(  A^{T}\right)  _{j,i}%
}_{\substack{=A_{i,j}\\\text{(by (\ref{pf.prop.transpose.invol.AT2}))}%
}}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}=\left(  A_{i,j}\right)  _{1\leq
i\leq m,\ 1\leq j\leq n}=A
\]
(by (\ref{eq.matrix.A-through-Aij})). This proves Proposition
\ref{prop.transpose.invol}.
\end{proof}

\subsection{Addition, scaling and multiplication}

Matrices can (sometimes) be added, (always) be scaled and (sometimes) be
multiplied. Let me explain:

\begin{definition}
Let $A$ and $B$ be two matrices of the same dimensions (that is, they have the
same number of rows, and the same number of columns). Then, $A+B$ denotes the
matrix obtained by adding each entry of $A$ to the corresponding entry of $B$.
Or, to write it more formally: If $A$ and $B$ are two $n\times m$-matrices,
then%
\[
A+B=\left(  A_{i,j}+B_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}.
\]

\end{definition}

For example, $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f
\end{array}
\right)  +\left(
\begin{array}
[c]{ccc}%
a^{\prime} & b^{\prime} & c^{\prime}\\
d^{\prime} & e^{\prime} & f^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
a+a^{\prime} & b+b^{\prime} & c+c^{\prime}\\
d+d^{\prime} & e+e^{\prime} & f+f^{\prime}%
\end{array}
\right)  $. (I am increasingly using variables instead of actual numbers in my
examples, because they make it easier to see what entry is going where.) On
the other hand, the two matrices $\left(
\begin{array}
[c]{c}%
a\\
b
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{cc}%
c & d
\end{array}
\right)  $ cannot be added (since they have different dimensions\footnote{The
dimensions of the former matrix are $2$ and $1$, whereas the dimensions of the
latter matrix are $1$ and $2$. Even though they are equal \textbf{up to
order}, they do not count as equal.}).

So we now know hot add two matrices.

\begin{definition}
Let $A$ be a matrix, and $\lambda$ be a number. Then, $\lambda A$ (or
$\lambda\cdot A$) denotes the matrix obtained by multiplying each entry of $A$
by $\lambda$. In other words: If $A$ is an $n\times m$-matrix, then%
\[
\lambda A=\left(  \lambda A_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}.
\]


We write $-A$ for $\left(  -1\right)  A$.
\end{definition}

For example, $\lambda\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
\lambda a & \lambda b & \lambda c\\
\lambda d & \lambda e & \lambda f
\end{array}
\right)  $.

So now we know how to scale a matrix. (\textquotedblleft To
scale\textquotedblright\ means to multiply by a number.)

\textquotedblleft Scaling\textquotedblright\ is often called \textquotedblleft
scalar multiplication\textquotedblright\ (but this is confusing terminology,
since \textquotedblleft scalar product\textquotedblright\ means something
completely different).

With scaling and addition defined, we obtain subtraction for free:

\begin{definition}
Let $A$ and $B$ be two matrices of the same dimensions. Then, $A-B$ denotes
the matrix $A+\left(  -B\right)  =A+\left(  -1\right)  B$.
\end{definition}

For example, $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f
\end{array}
\right)  -\left(
\begin{array}
[c]{ccc}%
a^{\prime} & b^{\prime} & c^{\prime}\\
d^{\prime} & e^{\prime} & f^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
a-a^{\prime} & b-b^{\prime} & c-c^{\prime}\\
d-d^{\prime} & e-e^{\prime} & f-f^{\prime}%
\end{array}
\right)  $.

Now, to the more interesting part: multiplying matrices. This is \textbf{not}
done by multiplying corresponding entries! (Why not? Well, it wouldn't make
for a particularly useful notion.) Instead, the definition goes as follows:

\begin{definition}
\label{def.AB}Let $n\in\mathbb{N}$, $m\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A$ be an $n\times m$-matrix. Let $B$ be an $m\times p$-matrix. (Thus, $A$ has
to have $m$ columns, while $B$ has to have $m$ rows; other than this, the two
matrices do not need to have any relation to each other.) The product $AB$ of
these two matrices is defined as follows:%
\[
AB=\left(  \underbrace{A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j}%
}_{\substack{\text{This is the sum of the }m\text{ terms of the form}%
\\A_{i,k}B_{k,j}\text{, for }k\text{ ranging over }\left\{  1,2,\ldots
,m\right\}  }}\right)  _{1\leq i\leq n,\ 1\leq j\leq p}.
\]
This is an $n\times p$-matrix.
\end{definition}

This definition is somewhat overwhelming, so let me rewrite it in words and
give some examples:

It says that the product $AB$ is well-defined whenever $A$ has as many columns
as $B$ has rows. In this case, $AB$ is the $n\times p$-matrix whose $\left(
i,j\right)  $-th entry is obtained by adding together:

\begin{itemize}
\item the product $A_{i,1}B_{1,j}$ of the $\left(  i,1\right)  $-th entry of
$A$ with the $\left(  1,j\right)  $-th entry of $B$;

\item the product $A_{i,2}B_{2,j}$ of the $\left(  i,2\right)  $-th entry of
$A$ with the $\left(  2,j\right)  $-th entry of $B$;

\item and so on;

\item the product $A_{i,m}B_{m,j}$ of the $\left(  i,m\right)  $-th entry of
$A$ with the $\left(  m,j\right)  $-th entry of $B$.
\end{itemize}

In other words, $AB$ is the matrix whose $\left(  i,j\right)  $-th entry is
obtained by multiplying each entry of the $i$-th row of $A$ with the
corresponding entry of the $j$-th column of $B$, and then adding together all
these products. The word \textquotedblleft corresponding\textquotedblright%
\ means that the $1$-st entry of the $i$-th row of $A$ gets multiplied with
the $1$-st entry of the $j$-th column of $B$, the $2$-nd entry with the $2$-nd
entry, etc.. In particular, for this to make sense, the $i$-th row of $A$ and
the $j$-th column of $B$ have to have the same number of entries. This is why
we required that $A$ has as many columns as $B$ has rows!

I promised examples. Here are four:%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
a & b\\
a^{\prime} & b^{\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
x & x^{\prime}\\
y & y^{\prime}%
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cc}%
ax+by & ax^{\prime}+by^{\prime}\\
a^{\prime}x+b^{\prime}y & a^{\prime}x^{\prime}+b^{\prime}y^{\prime}%
\end{array}
\right)  ;\\
\left(
\begin{array}
[c]{ccc}%
a & b & c\\
a^{\prime} & b^{\prime} & c^{\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
x\\
y\\
z
\end{array}
\right)   &  =\left(
\begin{array}
[c]{c}%
ax+by+cz\\
a^{\prime}x+b^{\prime}y+c^{\prime}z
\end{array}
\right)  ;\\
\left(
\begin{array}
[c]{cc}%
a & b
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
x\\
y
\end{array}
\right)   &  =\left(
\begin{array}
[c]{c}%
ax+by
\end{array}
\right)  ;\\
\left(
\begin{array}
[c]{c}%
a\\
b
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
x & y
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cc}%
ax & ay\\
bx & by
\end{array}
\right)
\end{align*}
(note how in the fourth example, we don't see any plus signs, because each of
the sums has only one addend).

We can also denote the product $AB$ by $A\cdot B$ (though few people ever do
this\footnote{\textbf{Warning:} The notation $A\cdot B$ is somewhat
nonstandard. Many authors (for example, Olver and Shakiban in \cite[\S 3.1]%
{OlvSha06}) define the \textquotedblleft dot product\textquotedblright\ of two
column vectors $v=\left(  v_{1},v_{2},\ldots,v_{n}\right)  ^{T}$ and
$w=\left(  w_{1},w_{2},\ldots,w_{n}\right)  ^{T}$ (of the same size) to be the
number $v_{1}w_{1}+v_{2}w_{2}+\cdots+v_{n}w_{n}$; they furthermore denote this
dot product by $v\cdot w$. This notation is in conflict with our notation
$A\cdot B$, because the dot product of $v$ and $w$ is not what we call $v\cdot
w$ (it is, in fact, what we call $v^{T}\cdot w$). The reason why I have picked
the somewhat nonstandard convention to regard $A\cdot B$ as a synonym for $AB$
is my belief that a dot should always denote the same multiplication as
juxtaposition (i.e., that $A\cdot B$ should always mean the same as $AB$).}).

We have thus learnt how to multiply matrices. Notice that the $\left(
i,j\right)  $-th entry of the product $AB$ depends only on the $i$-th row of
$A$ and the $j$-th column of $B$. Why did we pick this strange definition,
rather than something simpler, like multiplying entry by entry, or at least
row by row? Well, \textquotedblleft entry by entry\textquotedblright\ is too
simple (you will see later what matrix multiplication is good for;
\textquotedblleft entry by entry\textquotedblright\ is useless in comparison),
whereas \textquotedblleft row by row\textquotedblright\ would be lacking many
of the nice properties that we will see later (e.g., our matrix multiplication
satisfies the associativity law $\left(  AB\right)  C=A\left(  BC\right)  $,
while \textquotedblleft row by row\textquotedblright\ does not).

\begin{exercise}
\label{exe.AB-defined}Let $A=\left(
\begin{array}
[c]{cc}%
1 & -1\\
2 & 0\\
3 & 5
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cc}%
1 & 2\\
1 & 6
\end{array}
\right)  $.

\textbf{(a)} The matrix $A$ is of size $3\times2$. What is the size of $B$ ?

\textbf{(b)} Is $AB$ defined? If it is, compute it.

\textbf{(c)} Is $BA$ defined? If it is, compute it.
\end{exercise}

\begin{noncompile}
Exercise \ref{exe.AB-defined} is essentially from Quiz 1 in Lewis's.
\end{noncompile}

\begin{exercise}
\label{exe.some-prods}\textbf{(a)} Compute%
\[
\left(
\begin{array}
[c]{ccc}%
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)
\]
and%
\[
\left(
\begin{array}
[c]{cccc}%
1 & 1 & 1 & 1\\
0 & 1 & 1 & 1\\
0 & 0 & 1 & 1\\
0 & 0 & 0 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{cccc}%
1 & 1 & 1 & 1\\
0 & 1 & 1 & 1\\
0 & 0 & 1 & 1\\
0 & 0 & 0 & 1
\end{array}
\right)  .
\]


\textbf{(b)} Compute $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
a^{\prime} & b^{\prime} & c^{\prime}\\
a^{\prime\prime} & b^{\prime\prime} & c^{\prime\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
1 & 1 & 1\\
1 & 1 & 1\\
1 & 1 & 1
\end{array}
\right)  $ for an arbitrary $3\times3$-matrix $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
a^{\prime} & b^{\prime} & c^{\prime}\\
a^{\prime\prime} & b^{\prime\prime} & c^{\prime\prime}%
\end{array}
\right)  $.

\textbf{(c)} Compute $\left(
\begin{array}
[c]{cccc}%
1 & 1 & 1 & 1\\
0 & 1 & 1 & 1\\
0 & 0 & 1 & 1\\
0 & 0 & 0 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
a\\
b\\
c\\
d
\end{array}
\right)  $ for an arbitrary $4\times1$-matrix $\left(
\begin{array}
[c]{c}%
a\\
b\\
c\\
d
\end{array}
\right)  $.
\end{exercise}

\subsection{The matrix product rewritten}

Let me show another way to restate our above definition of a product of two
matrices. First, one more notation:

\begin{definition}
Let $A$ be an $n\times m$-matrix.

\textbf{(a)} If $i\in\left\{  1,2,\ldots,n\right\}  $, then
$\operatorname*{row}\nolimits_{i}A$ will denote the $i$-th row of $A$. This is
a row vector of size $m$ (that is, a $1\times m$-matrix), and is formally
defined as
\[
\left(  A_{i,y}\right)  _{1\leq x\leq1,\ 1\leq y\leq m}=\left(
\begin{array}
[c]{cccc}%
A_{i,1} & A_{i,2} & \cdots & A_{i,m}%
\end{array}
\right)
\]
(notice how $i$ is kept fixed but $y$ is ranging from $1$ to $m$ here).

\textbf{(b)} If $j\in\left\{  1,2,\ldots,m\right\}  $, then
$\operatorname*{col}\nolimits_{j}A$ will denote the $j$-th column of $A$. This
is a column vector of size $n$ (that is, an $n\times1$-matrix), and is
formally defined as
\[
\left(  A_{x,j}\right)  _{1\leq x\leq n,\ 1\leq y\leq1}=\left(
\begin{array}
[c]{c}%
A_{1,j}\\
A_{2,j}\\
\vdots\\
A_{n,j}%
\end{array}
\right)  .
\]

\end{definition}

\begin{example}
If $A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f
\end{array}
\right)  $, then $\operatorname*{row}\nolimits_{2}A=\left(
\begin{array}
[c]{ccc}%
d & e & f
\end{array}
\right)  $ and $\operatorname*{col}\nolimits_{2}A=\left(
\begin{array}
[c]{c}%
b\\
e
\end{array}
\right)  $.
\end{example}

Now, we observe that if $R$ is a row vector of some size $m$, and if $C$ is a
column vector of size $m$, then $RC$ is a $1\times1$-matrix. More precisely:
The product of a row vector $\left(
\begin{array}
[c]{cccc}%
r_{1} & r_{2} & \cdots & r_{m}%
\end{array}
\right)  $ and a column vector $\left(
\begin{array}
[c]{c}%
c_{1}\\
c_{2}\\
\vdots\\
c_{m}%
\end{array}
\right)  $ is given by
\begin{equation}
\left(
\begin{array}
[c]{cccc}%
r_{1} & r_{2} & \cdots & r_{m}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
c_{1}\\
c_{2}\\
\vdots\\
c_{m}%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
r_{1}c_{1}+r_{2}c_{2}+\cdots+r_{m}c_{m}%
\end{array}
\right)  . \label{eq.matrix-prod.RC}%
\end{equation}
We shall often equate a $1\times1$-matrix with its (unique) entry; so the
equality (\ref{eq.matrix-prod.RC}) rewrites as%
\begin{equation}
\left(
\begin{array}
[c]{cccc}%
r_{1} & r_{2} & \cdots & r_{m}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
c_{1}\\
c_{2}\\
\vdots\\
c_{m}%
\end{array}
\right)  =r_{1}c_{1}+r_{2}c_{2}+\cdots+r_{m}c_{m}. \label{eq.matrix-prod.RC.1}%
\end{equation}


Now I will show a little collection of formulas for the product of two
matrices. They are all pretty straightforward to obtain (essentially, they are
the definition of the product viewed from different angles), but they are
helpful when it comes to manipulating products:

\begin{proposition}
\label{prop.matrix-prod.rc}Let $n\in\mathbb{N}$, $m\in\mathbb{N}$ and
$p\in\mathbb{N}$. Let $A$ be an $n\times m$-matrix. Let $B$ be an $m\times p$-matrix.

\textbf{(a)} For every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,p\right\}  $, we have%
\[
\left(  AB\right)  _{i,j}=A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}%
B_{m,j}.
\]


\textbf{(b)} For every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,p\right\}  $, the $\left(  i,j\right)  $-th entry of $AB$ equals
the product of the $i$-th row of $A$ and the $j$-th column of $B$. In
formulas:%
\begin{equation}
\left(  AB\right)  _{i,j}=\operatorname*{row}\nolimits_{i}A\cdot
\operatorname*{col}\nolimits_{j}B \label{eq.prop.matrix-prod.rc.1}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,p\right\}  $ (where the expression $\operatorname*{row}%
\nolimits_{i}A\cdot\operatorname*{col}\nolimits_{j}B$ should be read as
$\left(  \operatorname*{row}\nolimits_{i}A\right)  \cdot\left(
\operatorname*{col}\nolimits_{j}B\right)  $). Thus,
\begin{align*}
AB  &  =\left(  \operatorname*{row}\nolimits_{i}A\cdot\operatorname*{col}%
\nolimits_{j}B\right)  _{1\leq i\leq n,\ 1\leq j\leq p}\\
&  =\left(
\begin{array}
[c]{cccc}%
\operatorname*{row}\nolimits_{1}A\cdot\operatorname*{col}\nolimits_{1}B &
\operatorname*{row}\nolimits_{1}A\cdot\operatorname*{col}\nolimits_{2}B &
\cdots & \operatorname*{row}\nolimits_{1}A\cdot\operatorname*{col}%
\nolimits_{p}B\\
\operatorname*{row}\nolimits_{2}A\cdot\operatorname*{col}\nolimits_{1}B &
\operatorname*{row}\nolimits_{2}A\cdot\operatorname*{col}\nolimits_{2}B &
\cdots & \operatorname*{row}\nolimits_{2}A\cdot\operatorname*{col}%
\nolimits_{p}B\\
\vdots & \vdots & \ddots & \vdots\\
\operatorname*{row}\nolimits_{n}A\cdot\operatorname*{col}\nolimits_{1}B &
\operatorname*{row}\nolimits_{n}A\cdot\operatorname*{col}\nolimits_{2}B &
\cdots & \operatorname*{row}\nolimits_{n}A\cdot\operatorname*{col}%
\nolimits_{p}B
\end{array}
\right)  .
\end{align*}


\textbf{(c)} For every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\[
\operatorname*{row}\nolimits_{i}\left(  AB\right)  =\left(
\operatorname*{row}\nolimits_{i}A\right)  \cdot B.
\]


\textbf{(d)} For every $j\in\left\{  1,2,\ldots,p\right\}  $, we have%
\[
\operatorname*{col}\nolimits_{j}\left(  AB\right)  =A\cdot\operatorname*{col}%
\nolimits_{j}B.
\]

\end{proposition}

Proposition \ref{prop.matrix-prod.rc} \textbf{(c)} says that if $A$ and $B$
are two matrices (for which $AB$ makes sense), then each row of $AB$ equals
the corresponding row of $A$ multiplied by $B$. Similarly, Proposition
\ref{prop.matrix-prod.rc} \textbf{(d)} says that each column of $AB$ equals
$A$ multiplied by the corresponding column of $B$. These are fairly simple
observations, but they are surprisingly useful.

\begin{proof}
[Proof of Proposition \ref{prop.matrix-prod.rc}.]\textbf{(a)} By the
definition of $AB$, we have%
\[
AB=\left(  A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq p}.
\]
In other words,%
\begin{equation}
\left(  AB\right)  _{i,j}=A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j}
\label{pf.prop.matrix-prod.rc.1}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,p\right\}  $. This proves Proposition \ref{prop.matrix-prod.rc}
\textbf{(a)}.

\textbf{(b)} Now, let us prove Proposition \ref{prop.matrix-prod.rc}
\textbf{(b)}. It is clearly enough to prove (\ref{eq.prop.matrix-prod.rc.1})
(because all the other statements of Proposition \ref{prop.matrix-prod.rc}
\textbf{(b)} are just restatements of (\ref{eq.prop.matrix-prod.rc.1})). So
let's do this. Let $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,p\right\}  $. Then,%
\begin{align}
\operatorname*{row}\nolimits_{i}A  &  =\left(
\begin{array}
[c]{cccc}%
A_{i,1} & A_{i,2} & \cdots & A_{i,m}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\label{pf.prop.matrix-prod.rc.row}\\
\operatorname*{col}\nolimits_{j}B  &  =\left(
\begin{array}
[c]{c}%
B_{1,j}\\
B_{2,j}\\
\vdots\\
B_{m,j}%
\end{array}
\right)  . \label{pf.prop.matrix-prod.rc.col}%
\end{align}
Hence,%
\begin{align*}
\operatorname*{row}\nolimits_{i}A\cdot\operatorname*{col}\nolimits_{j}B  &
=\left(
\begin{array}
[c]{cccc}%
A_{i,1} & A_{i,2} & \cdots & A_{i,m}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
B_{1,j}\\
B_{2,j}\\
\vdots\\
B_{m,j}%
\end{array}
\right) \\
&  =A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j}.
\end{align*}
Comparing this with (\ref{pf.prop.matrix-prod.rc.1}), we obtain $\left(
AB\right)  _{i,j}=\operatorname*{row}\nolimits_{i}A\cdot\operatorname*{col}%
\nolimits_{j}B$. Thus, we have proven (\ref{eq.prop.matrix-prod.rc.1}). Hence,
Proposition \ref{prop.matrix-prod.rc} \textbf{(b)} is proven.

\textbf{(c)} Let $i\in\left\{  1,2,\ldots,n\right\}  $. Set
$C=\operatorname*{row}\nolimits_{i}A$. Notice that $C$ is a row vector of size
$m$, thus a $1\times m$-matrix. We can refer to any given entry of $C$ either
as \textquotedblleft the $j$-th entry\textquotedblright\ or as
\textquotedblleft the $\left(  1,j\right)  $-th entry\textquotedblright%
\ (where $j$ is the number of the column the entry is located in).

We have%
\[
C=\operatorname*{row}\nolimits_{i}A=\left(
\begin{array}
[c]{cccc}%
A_{i,1} & A_{i,2} & \cdots & A_{i,m}%
\end{array}
\right)  .
\]
Thus,%
\begin{equation}
C_{1,k}=A_{i,k}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,\ldots,m\right\}  . \label{pf.prop.matrix-prod.rc.c.1}%
\end{equation}


Let $j\in\left\{  1,2,\ldots,p\right\}  $. Then,%
\begin{align*}
&  \left(  \text{the }j\text{-th entry of }\operatorname*{row}\nolimits_{i}%
\left(  AB\right)  \right) \\
&  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of }AB\right)
=\left(  AB\right)  _{i,j}\\
&  =A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.matrix-prod.rc.1})}\right)
.
\end{align*}
Comparing this with%
\begin{align*}
&  \left(  \text{the }j\text{-th entry of }CB\right) \\
&  =\left(  \text{the }\left(  1,j\right)  \text{-th entry of }CB\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }CB\text{ is a row vector}\right) \\
&  =\underbrace{C_{1,1}}_{\substack{=A_{1,1}\\\text{(by
(\ref{pf.prop.matrix-prod.rc.c.1}))}}}B_{1,j}+\underbrace{C_{1,2}%
}_{\substack{=A_{1,2}\\\text{(by (\ref{pf.prop.matrix-prod.rc.c.1}))}}%
}B_{2,j}+\cdots+\underbrace{C_{1,m}}_{\substack{=A_{1,m}\\\text{(by
(\ref{pf.prop.matrix-prod.rc.c.1}))}}}B_{m,j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.matrix-prod.rc} \textbf{(a)}, applied to
}1\text{, }C\text{ and }1\\
\text{instead of }n\text{, }A\text{ and }i
\end{array}
\right) \\
&  =A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j},
\end{align*}
we obtain%
\begin{equation}
\left(  \text{the }j\text{-th entry of }\operatorname*{row}\nolimits_{i}%
\left(  AB\right)  \right)  =\left(  \text{the }j\text{-th entry of
}CB\right)  . \label{pf.prop.matrix-prod.rc.c.5}%
\end{equation}


Now, forget that we fixed $j$. We thus have shown that
(\ref{pf.prop.matrix-prod.rc.c.5}) holds for each $j\in\left\{  1,2,\ldots
,p\right\}  $. In other words, each entry of the row vector
$\operatorname*{row}\nolimits_{i}\left(  AB\right)  $ equals the corresponding
entry of the row vector $CB$. Hence, $\operatorname*{row}\nolimits_{i}\left(
AB\right)  $ equals $CB$. Thus, $\operatorname*{row}\nolimits_{i}\left(
AB\right)  =\underbrace{C}_{=\operatorname*{row}\nolimits_{i}A}B=\left(
\operatorname*{row}\nolimits_{i}A\right)  \cdot B$. This proves Proposition
\ref{prop.matrix-prod.rc} \textbf{(c)}.

\textbf{(d)} The proof of Proposition \ref{prop.matrix-prod.rc} \textbf{(d)}
is similar to that of Proposition \ref{prop.matrix-prod.rc} \textbf{(c)}. Let
me nevertheless show it, for the sake of completeness. (The proof below is
essentially a copy-pasted version of the above proof of Proposition
\ref{prop.matrix-prod.rc} \textbf{(c)}, with only the necessary changes made.
This is both practical for me, as it saves me some work, and hopefully helpful
for you, as it highlights the similarities.)

Let $j\in\left\{  1,2,\ldots,p\right\}  $. Set $D=\operatorname*{col}%
\nolimits_{j}B$. Notice that $D$ is a column vector of size $m$, thus an
$m\times1$-matrix. We can refer to any given entry of $D$ either as
\textquotedblleft the $i$-th entry\textquotedblright\ or as \textquotedblleft
the $\left(  i,1\right)  $-th entry\textquotedblright\ (where $i$ is the
number of the row the entry is located in).

We have%
\[
D=\operatorname*{col}\nolimits_{j}B=\left(
\begin{array}
[c]{c}%
B_{1,j}\\
B_{2,j}\\
\vdots\\
B_{m,j}%
\end{array}
\right)  .
\]
Thus,%
\begin{equation}
D_{k,1}=B_{k,j}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,\ldots,m\right\}  . \label{pf.prop.matrix-prod.rc.d.1}%
\end{equation}


Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then,%
\begin{align*}
&  \left(  \text{the }i\text{-th entry of }\operatorname*{col}\nolimits_{j}%
\left(  AB\right)  \right) \\
&  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of }AB\right)
=\left(  AB\right)  _{i,j}\\
&  =A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.matrix-prod.rc.1})}\right)
.
\end{align*}
Comparing this with%
\begin{align*}
&  \left(  \text{the }i\text{-th entry of }AD\right) \\
&  =\left(  \text{the }\left(  i,1\right)  \text{-th entry of }AD\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }AD\text{ is a column vector}\right)
\\
&  =A_{i,1}\underbrace{D_{1,1}}_{\substack{=B_{1,j}\\\text{(by
(\ref{pf.prop.matrix-prod.rc.d.1}))}}}+A_{i,2}\underbrace{D_{2,1}%
}_{\substack{=B_{2,j}\\\text{(by (\ref{pf.prop.matrix-prod.rc.d.1}))}}%
}+\cdots+A_{i,m}\underbrace{D_{m,1}}_{\substack{=B_{m,j}\\\text{(by
(\ref{pf.prop.matrix-prod.rc.d.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.matrix-prod.rc} \textbf{(a)}, applied to
}1\text{, }D\text{ and }1\\
\text{instead of }p\text{, }B\text{ and }j
\end{array}
\right) \\
&  =A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j},
\end{align*}
we obtain%
\begin{equation}
\left(  \text{the }i\text{-th entry of }\operatorname*{col}\nolimits_{j}%
\left(  AB\right)  \right)  =\left(  \text{the }i\text{-th entry of
}AD\right)  . \label{pf.prop.matrix-prod.rc.d.5}%
\end{equation}


Now, forget that we fixed $i$. We thus have shown that
(\ref{pf.prop.matrix-prod.rc.d.5}) holds for each $i\in\left\{  1,2,\ldots
,n\right\}  $. In other words, each entry of the column vector
$\operatorname*{col}\nolimits_{j}\left(  AB\right)  $ equals the corresponding
entry of the column vector $AD$. Hence, $\operatorname*{col}\nolimits_{j}%
\left(  AB\right)  $ equals $AD$. Thus, $\operatorname*{col}\nolimits_{j}%
\left(  AB\right)  =A\underbrace{D}_{=\operatorname*{col}\nolimits_{j}%
B}=A\cdot\operatorname*{col}\nolimits_{j}B$. This proves Proposition
\ref{prop.matrix-prod.rc} \textbf{(d)}.
\end{proof}

\subsection{Properties of matrix operations}

The operations of adding, scaling and multiplying matrices, in many aspects,
\textquotedblleft behave almost as nicely as numbers\textquotedblright.
Specifically, I mean that they satisfy a bunch of laws that numbers satisfy:

\begin{proposition}
\label{prop.matrix-laws.1}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Then:

\textbf{(a)} We have $A+B=B+A$ for any two $n\times m$-matrices $A$ and $B$.
(This is called \textquotedblleft commutativity of addition\textquotedblright.)

\textbf{(b)} We have $A+\left(  B+C\right)  =\left(  A+B\right)  +C$ for any
three $n\times m$-matrices $A$, $B$ and $C$. (This is called \textquotedblleft
associativity of addition\textquotedblright.)

\textbf{(c)} We have $\lambda\left(  A+B\right)  =\lambda A+\lambda B$ for any
number $\lambda$ and any two $n\times m$-matrices $A$ and $B$.

Let furthermore $p\in\mathbb{N}$. Then:

\textbf{(d)} We have $A\left(  B+C\right)  =AB+AC$ for any $n\times m$-matrix
$A$ and any two $m\times p$-matrices $B$ and $C$. (This is called
\textquotedblleft left distributivity\textquotedblright.)

\textbf{(e)} We have $\left(  A+B\right)  C=AC+BC$ for any two $n\times
m$-matrices $A$ and $B$ and any $m\times p$-matrix $C$. (This is called
\textquotedblleft right distributivity\textquotedblright.)

\textbf{(f)} We have $\lambda\left(  AB\right)  =\left(  \lambda A\right)
B=A\left(  \lambda B\right)  $ for any number $\lambda$, any $n\times
m$-matrix $A$ and any $m\times p$-matrix $B$.

Finally, let $q\in\mathbb{N}$. Then:

\textbf{(g)} We have $A\left(  BC\right)  =\left(  AB\right)  C$ for any
$n\times m$-matrix $A$, any $m\times p$-matrix $B$ and any $p\times q$-matrix
$C$. (This is called \textquotedblleft associativity of
multiplication\textquotedblright.)
\end{proposition}

\begin{example}
Most parts of Proposition \ref{prop.matrix-laws.1} are fairly easy to
visualize and to prove. Let me give an example for the least obvious one: part
\textbf{(g)}.

Part \textbf{(g)} essentially says that $A\left(  BC\right)  =\left(
AB\right)  C$ holds for any three matrices $A$, $B$ and $C$ for which the
products $AB$ and $BC$ are well-defined (i.e., $A$ has as many columns as $B$
has rows, and $B$ has as many columns as $C$ has rows). For example, take
$n=1$, $m=3$, $p=2$ and $q=3$. Set%
\[
A=\left(
\begin{array}
[c]{ccc}%
a & b & c
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ B=\left(
\begin{array}
[c]{cc}%
d & d^{\prime}\\
e & e^{\prime}\\
f & f^{\prime}%
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ C=\left(
\begin{array}
[c]{ccc}%
x & y & z\\
x^{\prime} & y^{\prime} & z^{\prime}%
\end{array}
\right)  .
\]
Then,%
\[
AB=\left(
\begin{array}
[c]{cc}%
ad+be+cf & ad^{\prime}+be^{\prime}+cf^{\prime}%
\end{array}
\right)
\]
and thus%
\begin{align*}
\left(  AB\right)  C  &  =\left(
\begin{array}
[c]{cc}%
ad+be+cf & ad^{\prime}+be^{\prime}+cf^{\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
x & y & z\\
x^{\prime} & y^{\prime} & z^{\prime}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
ad^{\prime}x^{\prime}+be^{\prime}x^{\prime}+cf^{\prime}x^{\prime
}+bxe+adx+cfx\\
ad^{\prime}y^{\prime}+be^{\prime}y^{\prime}+cf^{\prime}y^{\prime
}+bye+ady+cfy\\
ad^{\prime}z^{\prime}+be^{\prime}z^{\prime}+cf^{\prime}z^{\prime}+bze+adz+cfz
\end{array}
\right)  ^{T}%
\end{align*}
after some computation. (Here, we have written the result as a transpose of a
column vector, because if we had written it as a row vector, it would not fit
on this page.) But
\[
BC=\left(
\begin{array}
[c]{cc}%
d & d^{\prime}\\
e & e^{\prime}\\
f & f^{\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
x & y & z\\
x^{\prime} & y^{\prime} & z^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
d^{\prime}x^{\prime}+dx & d^{\prime}y^{\prime}+dy & d^{\prime}z^{\prime}+dz\\
e^{\prime}x^{\prime}+xe & e^{\prime}y^{\prime}+ye & e^{\prime}z^{\prime}+ze\\
f^{\prime}x^{\prime}+fx & f^{\prime}y^{\prime}+fy & f^{\prime}z^{\prime}+fz
\end{array}
\right)
\]
and as before
\[
A\left(  BC\right)  =\left(
\begin{array}
[c]{c}%
ad^{\prime}x^{\prime}+be^{\prime}x^{\prime}+cf^{\prime}x^{\prime
}+bxe+adx+cfx\\
ad^{\prime}y^{\prime}+be^{\prime}y^{\prime}+cf^{\prime}y^{\prime
}+bye+ady+cfy\\
ad^{\prime}z^{\prime}+be^{\prime}z^{\prime}+cf^{\prime}z^{\prime}+bze+adz+cfz
\end{array}
\right)  ^{T}.
\]
Hence, $\left(  AB\right)  C=A\left(  BC\right)  $. Thus, our example confirms
Proposition \ref{prop.matrix-laws.1} \textbf{(g)}.
\end{example}

The laws of Proposition \ref{prop.matrix-laws.1} allow you to do many formal
manipulations with matrices similarly to how you are used to work with
numbers. For example, if you have $n$ matrices $A_{1},A_{2},\ldots,A_{n}$ such
that successive matrices can be multiplied (i.e., for each $i\in\left\{
1,2,\ldots,n-1\right\}  $, the matrix $A_{i}$ has as many columns as $A_{i+1}$
has rows), then the product $A_{1}A_{2}\cdots A_{n}$ is well-defined: you can
parenthesize it in any order, and the result will always be the same. For
example, the product $ABCD$ of four matrices $A,B,C,D$ can be computed in any
of the five ways%
\[
\left(  \left(  AB\right)  C\right)  D,\ \ \ \ \ \ \ \ \ \ \left(  AB\right)
\left(  CD\right)  ,\ \ \ \ \ \ \ \ \ \ \left(  A\left(  BC\right)  \right)
D,\ \ \ \ \ \ \ \ \ \ A\left(  \left(  BC\right)  D\right)
,\ \ \ \ \ \ \ \ \ \ A\left(  B\left(  CD\right)  \right)  ,
\]
and all of them lead to the same result. This is called \textit{general
associativity} and is not obvious (even if you know that Proposition
\ref{prop.matrix-laws.1} \textbf{(g)} holds)\footnote{If you are curious about
the proofs:
\par
We shall prove Proposition \ref{prop.matrix-laws.1} \textbf{(g)} further below
(in Section \ref{sect.intro.sum}). General associativity can be derived from
Proposition \ref{prop.matrix-laws.1} \textbf{(g)} in the general context of
\textquotedblleft binary operations\textquotedblright; see (for
example)\ \cite{Zuker14} for this argument.}.

Please take a moment to appreciate general associativity! Without it, we could
not make sense of products like $ABC$ and $ABCDE$, because their values could
depend on how we choose to compute them. This is one reason why, in the
definition of $AB$, we multiply entries of the $i$-th row of $A$ with entries
of the $j$-th column of $B$. Using rows both times would break associativity!

We shall give proofs of parts \textbf{(d)} and \textbf{(g)} of Proposition
\ref{prop.matrix-laws.1} in Section \ref{sect.intro.sum} below.

\subsection{Non-properties of matrix operations}

Conspicuously absent from Proposition \ref{prop.matrix-laws.1} is one
important law that is well-known to hold for numbers: commutativity of
multiplication (that is, $ab=ba$). This has a reason: it is false for
matrices. There are at least three reasons why it is false:

\begin{enumerate}
\item If $A$ and $B$ are matrices, then it can happen that $AB$ is
well-defined (i.e., $A$ has as many columns as $B$ has rows) but $BA$ is not
(i.e., $B$ does not have as many columns as $A$ has rows). For example, if
$A=\left(
\begin{array}
[c]{c}%
a\\
b\\
c
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cc}%
x & y
\end{array}
\right)  $, then $AB$ is well-defined but $BA$ is not.

\item If $A$ and $B$ are matrices such that both $AB$ and $BA$ well-defined,
then $AB$ and $BA$ might still have different dimensions. Namely, if $A$ is an
$n\times m$-matrix and $B$ is an $m\times n$-matrix, then $AB$ is an $n\times
n$-matrix, but $BA$ is an $m\times m$-matrix. So comparing $AB$ and $BA$ makes
no sense unless $n=m$.

\item Even if $AB$ and $BA$ are of the same dimensions, they can still be
distinct. For example, if $A=\left(
\begin{array}
[c]{cc}%
1 & 1\\
0 & 1
\end{array}
\right)  $ and $B=A^{T}=\left(
\begin{array}
[c]{cc}%
1 & 0\\
1 & 1
\end{array}
\right)  $, then $AB=\left(
\begin{array}
[c]{cc}%
2 & 1\\
1 & 1
\end{array}
\right)  $ whereas $BA=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 2
\end{array}
\right)  $.
\end{enumerate}

Two matrices $A$ and $B$ are said to \textit{commute} if $AB=BA$ (which, in
particular, means that both $AB$ and $BA$ are well-defined). You will
encounter many cases when matrices $A$ and $B$ happen to commute (for example,
every $n\times n$-matrix commutes with the $n\times n$ identity matrix; see
below for what this means); but in general there is no reason to expect two
randomly chosen matrices to commute.

As a consequence of matrices refusing to commute (in general), we cannot
reasonably define division of matrices. Actually, there are two reasons why we
cannot reasonably define division of matrices: First, if $A$ and $B$ are two
matrices, then it is not clear whether $\dfrac{A}{B}$ should mean a matrix $C$
satisfying $BC=A$, or a matrix $C$ satisfying $CB=A$. (The failure of
commutativity implies that these are two different things.) Second, in
general, neither of these matrices $C$ is necessarily unique; nor is it
guaranteed to exist. This is similar to the fact that we cannot divide by $0$
(in fact, $\dfrac{0}{0}$ would not be unique, while $\dfrac{1}{0}$ would not
exist); but with matrices, $0$ is not the only forbidden denominator. Here is
an example:

\begin{example}
\textbf{(a)} Let $A=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 0
\end{array}
\right)  $ and $B=A$. Then, $BC=A$ holds for $C=A$, but also for $C=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  $ (and also for many other matrices $C$). So the matrix $C$
satisfying $BC=A$ is not unique.

\textbf{(b)} Let $A=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 0
\end{array}
\right)  $. Then, there exists no matrix $C$ satisfing $BC=A$. Indeed, if
$C=\left(
\begin{array}
[c]{cc}%
x & y\\
z & w
\end{array}
\right)  $ is any matrix, then $BC=\left(
\begin{array}
[c]{cc}%
x & y\\
0 & 0
\end{array}
\right)  $ has its second row filled with zeroes, but $A$ does not; so $BC$
cannot equal $A$.
\end{example}

\begin{exercise}
\label{exe.commutativity-example}\textbf{(a)} Let $A=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 2
\end{array}
\right)  $. Show that the $2\times2$-matrices $B$ satisfying $AB=BA$ are
precisely the matrices of the form $\left(
\begin{array}
[c]{cc}%
a & 0\\
0 & d
\end{array}
\right)  $ (where $a$ and $d$ are any numbers). [\textbf{Hint:} Set $B=\left(
%
\begin{array}
[c]{cc}%
x & y\\
z & w
\end{array}
\right)  $, and rewrite $AB=BA$ as a system of linear equations in $x,y,z,w$.
Solve this system.]

\textbf{(b)} Let $A=\left(
\begin{array}
[c]{cc}%
1 & 1\\
0 & 1
\end{array}
\right)  $. Characterize the $2\times2$-matrices $B$ satisfying $AB=BA$.
\end{exercise}

\subsection{\label{sect.intro.sum}(*) The summation sign, and a proof of
$\left(  AB\right)  C=A\left(  BC\right)  $}

We now take a break from studying matrices to introduce an important symbol:
the summation sign ($\sum$). This sign is one of the hallmarks of abstract
mathematics (and also computer science), and helps manipulate matrices comfortably.

\begin{definition}
\label{def.sum}Let $p$ and $q$ be two integers such that $p\leq q$. Let
$a_{p},a_{p+1},\ldots,a_{q}$ be some numbers. Then, $\sum_{k=p}^{q}a_{k}$
means the sum $a_{p}+a_{p+1}+\cdots+a_{q}$. The symbol $\sum$ is called the
summation sign; we pronounce the expression $\sum_{k=p}^{q}a_{k}$ as
\textquotedblleft sum of $a_{k}$ for all $k$ ranging from $p$ to
$q$\textquotedblright.
\end{definition}

Here are some examples:

\begin{itemize}
\item We have $\sum_{k=p}^{q}k=p+\left(  p+1\right)  +\cdots+q$. For example,
$\sum_{k=1}^{n}k=1+2+\cdots+n$. (A well-known formula says that this sum
$\sum_{k=1}^{n}k=1+2+\cdots+n$ equals $\dfrac{n\left(  n+1\right)  }{2}$. For
a concrete example, $\sum_{k=1}^{3}k=1+2+3=\dfrac{3\left(  3+1\right)  }{2}%
=6$.) For another example, $\sum_{k=-n}^{n}k=\left(  -n\right)  +\left(
-n+1\right)  +\cdots+n$. (This latter sum equals $0$, because it contains, for
each its addend, also its negative\footnote{except for the addend $0$, but
this $0$ doesn't change the sum anyway}.)

\item We have $\sum_{k=p}^{q}k^{2}=p^{2}+\left(  p+1\right)  ^{2}+\cdots
+q^{2}$. For example, $\sum_{k=1}^{n}k^{2}=1^{2}+2^{2}+\cdots+n^{2}$. (A
well-known formula says that this sum $\sum_{k=1}^{n}k^{2}=1^{2}+2^{2}%
+\cdots+n^{2}$ equals $\dfrac{n\left(  n+1\right)  \left(  2n+1\right)  }{6}$.)

\item If $p=q$, then $\sum_{k=p}^{q}a_{k}=a_{p}$. (A sum of only one number is
simply this number.)
\end{itemize}

Notice that the expression \textquotedblleft$\sum_{k=p}^{q}a_{k}%
$\textquotedblright\ is both a more compact and a more rigorous way to say
\textquotedblleft$a_{p}+a_{p+1}+\cdots+a_{q}$\textquotedblright. A computer
would not understand the expression \textquotedblleft$a_{p}+a_{p+1}%
+\cdots+a_{q}$\textquotedblright\ (it could only guess what the
\textquotedblleft$\cdots$\textquotedblright\ means, and computers are bad at
guessing); but the expression \textquotedblleft$\sum_{k=p}^{q}a_{k}%
$\textquotedblright\ has a well-defined meaning that can be rigorously defined
and can be explained to a computer\footnote{Of course, the way we defined it
above is not rigorous. Here is how to define it rigorously (if you know
recursion):
\par
\begin{itemize}
\item If $q=p$, then $\sum_{k=p}^{q}a_{k}$ is defined to be $a_{p}$.
\par
\item If $q>p$, then $\sum_{k=p}^{q}a_{k}$ is defined to be $\left(
\sum_{k=p}^{q-1}a_{k}\right)  +a_{q}$.
\end{itemize}
\par
This is a recursive definition (since it defines $\sum_{k=p}^{q}a_{k}$ in
terms of $\sum_{k=p}^{q-1}a_{k}$), and provides an algorithm to compute
$\sum_{k=p}^{q}a_{k}$.}. Thus, if you want to tell a computer to compute a
sum, the command you have to use will be closer to \textquotedblleft%
$\sum_{k=p}^{q}a_{k}$\textquotedblright\ than to \textquotedblleft%
$a_{p}+a_{p+1}+\cdots+a_{q}$\textquotedblright. For example, in Python, you
would have to write \textquotedblleft\texttt{sum(a[k] for k in range(p,
q+1))}\textquotedblright\ (where \textquotedblleft\texttt{a[k]}%
\textquotedblright\ is understood to return $a_{k}$)\ \ \ \ \footnote{Why
\textquotedblleft\texttt{q+1}\textquotedblright\ and not \textquotedblleft%
\texttt{q}\textquotedblright? Because Python defines \texttt{range(u, v)} as
the list $\left(  u,u+1,\ldots,v-1\right)  $ (that is, the list that starts at
$u$ and ends \textbf{just before} $v$). So \texttt{range(p, q)} would be
$\left(  p,p+1,\ldots,q-1\right)  $, but we want $\left(  p,p+1,\ldots
,q\right)  $.}.

A few remarks on the summation sign are in order:

\begin{definition}
\label{def.sum.2}\textbf{(a)} In the expression $\sum_{k=p}^{q}a_{k}$ (as
defined in Definition \ref{def.sum}), the letter $k$ is called the
\textit{summation index}. It doesn't have to be called $k$; any letter is
legitimate (as long as it is not already used otherwise). For example,
$\sum_{i=p}^{q}a_{i}$ and $\sum_{x=p}^{q}a_{x}$ are two synonymous ways to
write $\sum_{k=p}^{q}a_{k}$. Just make sure that the you are using the same
letter under the $\sum$ sign and to its right (so you should not write
$\sum_{i=p}^{q}a_{k}$, unless you mean the sum $\underbrace{a_{k}+a_{k}%
+\cdots+a_{k}}_{q-p+1\text{ times}}$).

\textbf{(b)} The sum $\sum_{k=p}^{q}a_{k}$ is also defined when $p>q$. In this
case, it is called an \textit{empty sum} (because there are no numbers
$a_{p},a_{p+1},\ldots,a_{q}$) and defined to be $0$. This convention might
sound somewhat arbitrary, but it is the most reasonable way to define empty sums.
\end{definition}

Using the summation sign, we can rewrite the product $AB$ of two matrices $A$
and $B$ (see Definition \ref{def.AB}) more nicely:

\begin{proposition}
\label{prop.AB}Let $n,m,p$ be three integers. Let $A$ be an $n\times
m$-matrix. Let $B$ be an $m\times p$-matrix. Then,%
\[
\left(  AB\right)  _{i,j}=\sum_{k=1}^{m}A_{i,k}B_{k,j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,\ldots,n\right\}  \text{
and }j\in\left\{  1,2,\ldots,p\right\}  .
\]

\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.AB}.]For all $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,p\right\}  $, we have%
\begin{align*}
\left(  AB\right)  _{i,j}  &  =A_{i,1}B_{1,j}+A_{i,2}B_{2,j}+\cdots
+A_{i,m}B_{m,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.matrix-prod.rc} \textbf{(a)}}\right) \\
&  =\sum_{k=1}^{m}A_{i,k}B_{k,j}%
\end{align*}
(because $\sum_{k=1}^{m}A_{i,k}B_{k,j}$ is exactly $A_{i,1}B_{1,j}%
+A_{i,2}B_{2,j}+\cdots+A_{i,m}B_{m,j}$, by its definition). Proposition
\ref{prop.AB} is proven.
\end{proof}

Here are two properties of sums that are fairly clear if you understand how
sums are defined:

\begin{proposition}
\label{prop.sum.dist}Let $p$ and $q$ be two integers such that $p\leq q$. Let
$a_{p},a_{p+1},\ldots,a_{q}$ be some numbers. Let $b$ be a number. Then,
\[
\sum_{k=p}^{q}ba_{k}=b\sum_{k=p}^{q}a_{k}.
\]
(The expression $\sum_{k=p}^{q}ba_{k}$ has to be read as $\sum_{k=p}%
^{q}\left(  ba_{k}\right)  $.)
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.sum.dist}.]By the definition of $\sum$, we
have%
\[
\sum_{k=p}^{q}ba_{k}=ba_{p}+ba_{p+1}+\cdots+ba_{q}=b\underbrace{\left(
a_{p}+a_{p+1}+\cdots+a_{q}\right)  }_{\substack{=\sum_{k=p}^{q}a_{k}%
\\\text{(by the definition of }\sum_{k=p}^{q}a_{k}\text{)}}}=b\sum_{k=p}%
^{q}a_{k}.
\]

\end{proof}

\begin{proposition}
\label{prop.sum.a+b}Let $p$ and $q$ be two integers such that $p\leq q$. Let
$a_{p},a_{p+1},\ldots,a_{q}$ be some numbers. Let $b_{p},b_{p+1},\ldots,b_{q}$
be some numbers. Then,
\[
\sum_{k=p}^{q}\left(  a_{k}+b_{k}\right)  =\sum_{k=p}^{q}a_{k}+\sum_{k=p}%
^{q}b_{k}.
\]
(The expression $\sum_{k=p}^{q}a_{k}+\sum_{k=p}^{q}b_{k}$ has to be read as
$\left(  \sum_{k=p}^{q}a_{k}\right)  +\left(  \sum_{k=p}^{q}b_{k}\right)  $.)
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.sum.dist}.]By the definition of $\sum$, we
have%
\begin{align*}
\sum_{k=p}^{q}\left(  a_{k}+b_{k}\right)   &  =\left(  a_{p}+b_{p}\right)
+\left(  a_{p+1}+b_{p+1}\right)  +\cdots+\left(  a_{q}+b_{q}\right) \\
&  =\underbrace{\left(  a_{p}+a_{p+1}+\cdots+a_{q}\right)  }_{\substack{=\sum
_{k=p}^{q}a_{k}\\\text{(by the definition of }\sum_{k=p}^{q}a_{k}\text{)}%
}}+\underbrace{\left(  b_{p}+b_{p+1}+\cdots+b_{q}\right)  }_{\substack{=\sum
_{k=p}^{q}b_{k}\\\text{(by the definition of }\sum_{k=p}^{q}b_{k}\text{)}}}\\
&  =\sum_{k=p}^{q}a_{k}+\sum_{k=p}^{q}b_{k}.
\end{align*}

\end{proof}

Our goal in this section is to prove Proposition \ref{prop.matrix-laws.1}
\textbf{(g)}, illustrating the use and manipulation of the $\sum$ sign.
However, as a warmup, let us first prove Proposition \ref{prop.matrix-laws.1}
\textbf{(d)} (which is simple enough that you can easily check it without
$\sum$ signs, but is nevertheless worth proving using the $\sum$ sign just to
demonstrate how to work with the $\sum$ sign):

\begin{proof}
[Proof of Proposition \ref{prop.matrix-laws.1} \textbf{(d)}.]Let $A$ be an
$n\times m$-matrix. Let $B$ and $C$ be two $m\times p$-matrices.

We shall show that $\left(  A\left(  B+C\right)  \right)  _{i,j}=\left(
AB+AC\right)  _{i,j}$ for all $i\in\left\{  1,2,\ldots,n\right\}  $ and
$j\in\left\{  1,2,\ldots,p\right\}  $. Once this is proven, this will entail
that corresponding entries of the two $n\times p$-matrices $A\left(
B+C\right)  $ and $AB+AC$ are equal; and thus, these two matrices have to be equal.

Proposition \ref{prop.AB} yields%
\begin{equation}
\left(  AB\right)  _{i,j}=\sum_{k=1}^{m}A_{i,k}B_{k,j}
\label{pf.prop.matrix-laws.1.d.1}%
\end{equation}
for all $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,p\right\}  $.

Proposition \ref{prop.AB} (applied to $C$ instead of $B$) yields%
\begin{equation}
\left(  AC\right)  _{i,j}=\sum_{k=1}^{m}A_{i,k}C_{k,j}
\label{pf.prop.matrix-laws.1.d.2}%
\end{equation}
for all $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,p\right\}  $.

Finally, Proposition \ref{prop.AB} (applied to $B+C$ instead of $B$) yields%
\begin{align*}
\left(  A\left(  B+C\right)  \right)  _{i,j}  &  =\sum_{k=1}^{m}%
A_{i,k}\underbrace{\left(  B+C\right)  _{k,j}}_{\substack{=B_{k,j}%
+C_{k,j}\\\text{(since matrices are}\\\text{added entry by entry)}}%
}=\sum_{k=1}^{m}\underbrace{A_{i,k}\left(  B_{k,j}+C_{k,j}\right)  }%
_{=A_{i,k}B_{k,j}+A_{i,k}C_{k,j}}\\
&  =\sum_{k=1}^{m}\left(  A_{i,k}B_{k,j}+A_{i,k}C_{k,j}\right)
=\underbrace{\sum_{k=1}^{m}A_{i,k}B_{k,j}}_{\substack{=\left(  AB\right)
_{i,j}\\\text{(by (\ref{pf.prop.matrix-laws.1.d.1}))}}}+\underbrace{\sum
_{k=1}^{m}A_{i,k}C_{k,j}}_{\substack{=\left(  AC\right)  _{i,j}\\\text{(by
(\ref{pf.prop.matrix-laws.1.d.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.sum.a+b}, applied to }p=1\text{, }q=m\text{,}\\
a_{k}=A_{i,k}B_{k,j}\text{ and }b_{k}=A_{i,k}C_{k,j}%
\end{array}
\right) \\
&  =\left(  AB\right)  _{i,j}+\left(  AC\right)  _{i,j}\\
&  =\left(  AB+AC\right)  _{i,j}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{again because matrices}\\
\text{are added entry by entry}%
\end{array}
\right)
\end{align*}
for all $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,p\right\}  $. In other words, each entry of the $n\times p$-matrix $A\left(
B+C\right)  $ equals the corresponding entry of $AB+AC$. Thus, the matrix
$A\left(  B+C\right)  $ equals $AB+AC$. This proves Proposition
\ref{prop.matrix-laws.1} \textbf{(d)}.
\end{proof}

Before we can prove Proposition \ref{prop.matrix-laws.1} \textbf{(g)}, we need
another fact about sums:

\begin{proposition}
\label{prop.sum.ij}Let $m\in\mathbb{N}$ and $p\in\mathbb{N}$. Assume that a
number $a_{k,\ell}$ is given for every $k\in\left\{  1,2,\ldots,m\right\}  $
and $\ell\in\left\{  1,2,\ldots,p\right\}  $. Then,%
\[
\sum_{k=1}^{m}\sum_{\ell=1}^{p}a_{k,\ell}=\sum_{\ell=1}^{p}\sum_{k=1}%
^{m}a_{k,\ell}.
\]
(Note that an expression like $\sum_{k=1}^{m}\sum_{\ell=1}^{p}a_{k,\ell}$ has
to be understood as $\sum_{k=1}^{m}\left(  \sum_{\ell=1}^{p}a_{k,\ell}\right)
$. It is a \textquotedblleft nested sum\textquotedblright, i.e., a sum of
sums. For example,%
\begin{align*}
\sum_{k=1}^{3}\underbrace{\sum_{\ell=1}^{4}k\cdot\ell}_{=k\cdot1+k\cdot
2+k\cdot3+k\cdot4}  &  =\sum_{k=1}^{3}\left(  k\cdot1+k\cdot2+k\cdot
3+k\cdot4\right) \\
&  =\left(  1\cdot1+1\cdot2+1\cdot3+1\cdot4\right)  +\left(  2\cdot
1+2\cdot2+2\cdot3+2\cdot4\right)  s\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  3\cdot1+3\cdot2+3\cdot3+3\cdot4\right)  .
\end{align*}
)
\end{proposition}

\begin{example}
For $m=2$ and $p=3$, Proposition \ref{prop.sum.ij} says that%
\[
\sum_{k=1}^{2}\sum_{\ell=1}^{3}a_{k,\ell}=\sum_{\ell=1}^{3}\sum_{k=1}%
^{2}a_{k,\ell}.
\]
In other words,%
\[
\left(  a_{1,1}+a_{1,2}+a_{1,3}\right)  +\left(  a_{2,1}+a_{2,2}%
+a_{2,3}\right)  =\left(  a_{1,1}+a_{2,1}\right)  +\left(  a_{1,2}%
+a_{2,2}\right)  +\left(  a_{1,3}+a_{2,3}\right)  .
\]

\end{example}

\begin{proof}
[Proof of Proposition \ref{prop.sum.ij}.]Comparing%
\begin{align*}
&  \sum_{k=1}^{m}\underbrace{\sum_{\ell=1}^{p}a_{k,\ell}}_{\substack{=a_{k,1}%
+a_{k,2}+\cdots+a_{k,p}\\\text{(by the definition of the }\sum\text{ sign)}%
}}\\
&  =\sum_{k=1}^{m}\left(  a_{k,1}+a_{k,2}+\cdots+a_{k,p}\right) \\
&  =\left(  a_{1,1}+a_{1,2}+\cdots+a_{1,p}\right)  +\left(  a_{2,1}%
+a_{2,2}+\cdots+a_{2,p}\right)  +\cdots+\left(  a_{m,1}+a_{m,2}+\cdots
+a_{m,p}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the }\sum\text{
sign}\right) \\
&  =\left(  \text{the sum of all possible numbers }a_{k,\ell}\right)
\end{align*}
with%
\begin{align*}
&  \sum_{\ell=1}^{p}\underbrace{\sum_{k=1}^{m}a_{k,\ell}}%
_{\substack{=a_{1,\ell}+a_{2,\ell}+\cdots+a_{m,\ell}\\\text{(by the definition
of the }\sum\text{ sign)}}}\\
&  =\sum_{\ell=1}^{p}\left(  a_{1,\ell}+a_{2,\ell}+\cdots+a_{m,\ell}\right) \\
&  =\left(  a_{1,1}+a_{2,1}+\cdots+a_{m,1}\right)  +\left(  a_{1,2}%
+a_{2,2}+\cdots+a_{m,2}\right)  +\cdots+\left(  a_{1,p}+a_{2,p}+\cdots
+a_{m,p}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the }\sum\text{
sign}\right) \\
&  =\left(  \text{the sum of all possible numbers }a_{k,\ell}\right)  ,
\end{align*}
we obtain $\sum_{k=1}^{m}\left(  \sum_{\ell=1}^{p}a_{k,\ell}\right)
=\sum_{\ell=1}^{p}\left(  \sum_{k=1}^{m}a_{k,\ell}\right)  $.

(A more rigorous proof could be given using induction; but this one will
suffice for our purposes. Notice the visual meaning of the above proof: If we
place the $mp$ numbers $a_{k,\ell}$ into a matrix $\left(  a_{k,\ell}\right)
_{1\leq k\leq m,\ 1\leq\ell\leq p}$, then

\begin{itemize}
\item the number $\sum_{k=1}^{m}\sum_{\ell=1}^{p}a_{k,\ell}$ is obtained by
summing the entries in each row of the matrix, and then summing the resulting sums;

\item the number $\sum_{\ell=1}^{p}\sum_{k=1}^{m}a_{k,\ell}$ is obtained by
summing the entries in each column of the matrix, and then summing the
resulting sums.
\end{itemize}

Thus, clearly, both numbers are equal (namely, equal to the sum of all entries
of the matrix).)
\end{proof}

Now, we can prove Proposition \ref{prop.matrix-laws.1} \textbf{(g)}:

\begin{proof}
[Proof of Proposition \ref{prop.matrix-laws.1} \textbf{(g)}.]Let $A$ be an
$n\times m$-matrix. Let $B$ be an $m\times p$-matrix. Let $C$ be a $p\times q$-matrix.

We must show that $A\left(  BC\right)  =\left(  AB\right)  C$. In order to do
so, it suffices to show that $\left(  A\left(  BC\right)  \right)
_{i,j}=\left(  \left(  AB\right)  C\right)  _{i,j}$ for all $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,q\right\}  $ (because
this will show that respective entries of the two $n\times q$-matrices
$A\left(  BC\right)  $ and $\left(  AB\right)  C$ are equal, and thus the two
matrices are equal).

We know that $A$ is an $n\times m$-matrix, and that $BC$ is an $m\times
q$-matrix. Hence, we can apply Proposition \ref{prop.AB} to $n$, $m$, $q$, $A$
and $BC$ instead of $n$, $m$, $p$, $A$ and $B$. We thus obtain%
\begin{equation}
\left(  A\left(  BC\right)  \right)  _{i,j}=\sum_{k=1}^{m}A_{i,k}\left(
BC\right)  _{k,j} \label{pf.prop.matrix-laws.1.g.A(BC)}%
\end{equation}
for all $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,q\right\}  $. Also, we can apply Proposition \ref{prop.AB} to $m$, $p$, $q$,
$B$ and $C$ instead of $n$, $m$, $p$, $A$ and $B$. We thus obtain%
\begin{align}
\left(  BC\right)  _{i,j}  &  =\sum_{k=1}^{p}B_{i,k}C_{k,j}=\sum_{\ell=1}%
^{p}B_{i,\ell}C_{\ell,j}\label{pf.prop.matrix-laws.1.g.BC}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }\ell\right) \nonumber
\end{align}
for all $i\in\left\{  1,2,\ldots,m\right\}  $ and $j\in\left\{  1,2,\ldots
,p\right\}  $. Furthermore, we can apply Proposition \ref{prop.AB} to $n$,
$p$, $q$, $AB$ and $C$ instead of $n$, $m$, $p$, $A$ and $B$. We thus find%
\begin{align}
\left(  \left(  AB\right)  C\right)  _{i,j}  &  =\sum_{k=1}^{p}\left(
AB\right)  _{i,k}C_{k,j}=\sum_{\ell=1}^{p}\left(  AB\right)  _{i,\ell}%
C_{\ell,j}\label{pf.prop.matrix-laws.1.g.(AB)C}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }\ell\right) \nonumber
\end{align}
for all $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,q\right\}  $. Finally, Proposition \ref{prop.AB} (applied verbatim) yields%
\begin{equation}
\left(  AB\right)  _{i,j}=\sum_{k=1}^{m}A_{i,k}B_{k,j}
\label{pf.prop.matrix-laws.1.g.AB}%
\end{equation}
for all $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,p\right\}  $.

Now that we have found formulas for the entries of all matrices involved, we
can perform our computation: For all $i\in\left\{  1,2,\ldots,n\right\}  $ and
$j\in\left\{  1,2,\ldots,q\right\}  $, we have%
\begin{align*}
\left(  A\left(  BC\right)  \right)  _{i,j}  &  =\sum_{k=1}^{m}A_{i,k}%
\underbrace{\left(  BC\right)  _{k,j}}_{\substack{=\sum_{\ell=1}^{p}B_{k,\ell
}C_{\ell,j}\\\text{(by (\ref{pf.prop.matrix-laws.1.g.BC}), applied to }k\text{
instead of }i\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.matrix-laws.1.g.(AB)C})}\right) \\
&  =\sum_{k=1}^{m}\underbrace{A_{i,k}\left(  \sum_{\ell=1}^{p}B_{k,\ell
}C_{\ell,j}\right)  }_{\substack{=\sum_{\ell=1}^{p}A_{i,k}B_{k,\ell}C_{\ell
,j}\\\text{(by an application of}\\\text{Proposition \ref{prop.sum.dist})}%
}}=\sum_{k=1}^{m}\sum_{\ell=1}^{p}A_{i,k}B_{k,\ell}C_{\ell,j}\\
&  =\sum_{\ell=1}^{p}\sum_{k=1}^{m}A_{i,k}B_{k,\ell}C_{\ell,j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.sum.ij}, applied to
}a_{k,\ell}=A_{i,k}B_{k,\ell}C_{\ell,j}\right)
\end{align*}
and%
\begin{align*}
\left(  \left(  AB\right)  C\right)  _{i,j}  &  =\sum_{\ell=1}^{p}%
\underbrace{\left(  AB\right)  _{i,\ell}}_{\substack{=\sum_{k=1}^{m}%
A_{i,k}B_{k,\ell}\\\text{(by (\ref{pf.prop.matrix-laws.1.g.AB}), applied to
}\ell\\\text{instead of }j\text{)}}}C_{\ell,j}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.prop.matrix-laws.1.g.A(BC)})}\right) \\
&  =\sum_{k=1}^{m}\underbrace{\left(  \sum_{k=1}^{m}A_{i,k}B_{k,\ell}\right)
C_{\ell,j}}_{\substack{=C_{\ell,j}\sum_{k=1}^{m}A_{i,k}B_{k,\ell}=\sum
_{\ell=1}^{p}C_{\ell,j}A_{i,k}B_{k,\ell}\\\text{(by an application
of}\\\text{Proposition \ref{prop.sum.dist})}}}=\sum_{k=1}^{m}\sum_{\ell=1}%
^{p}\underbrace{C_{\ell,j}A_{i,k}B_{k,\ell}}_{=A_{i,k}B_{k,\ell}C_{\ell,j}}\\
&  =\sum_{\ell=1}^{p}\sum_{k=1}^{m}A_{i,k}B_{k,\ell}C_{\ell,j}.
\end{align*}
Comparing these two equalities, we obtain
\[
\left(  A\left(  BC\right)  \right)  _{i,j}=\left(  \left(  AB\right)
C\right)  _{i,j}%
\]
for all $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,q\right\}  $. In other words, each entry of the matrix $A\left(  BC\right)  $
equals the corresponding entry of the matrix $\left(  AB\right)  C$. Thus, the
matrices $A\left(  BC\right)  $ and $\left(  AB\right)  C$ are equal. This
proves Proposition \ref{prop.matrix-laws.1} \textbf{(g)}.
\end{proof}

We have just proven the hardest part of Proposition \ref{prop.matrix-laws.1}.
The rest is fairly straightforward.

\subsection{The zero matrix}

\begin{definition}
Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Then, the $n\times m$\textit{ zero
matrix} means the matrix $\left(  0\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.
This is the $n\times m$-matrix filled with zeroes. It is called $0_{n\times
m}$. (When no confusion with the number $0$ can arise, we will just call it
$0$.)
\end{definition}

For example, the $2\times3$ zero matrix is $\left(
\begin{array}
[c]{ccc}%
0 & 0 & 0\\
0 & 0 & 0
\end{array}
\right)  $.

The zero matrix behaves very much like the number $0$:

\begin{proposition}
\label{prop.matrix-laws.0}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Then:

\textbf{(a)} We have $0_{n\times m}+A=A+0_{n\times m}=A$ for each $n\times
m$-matrix $A$.

\textbf{(b)} We have $0_{n\times m}A=0_{n\times p}$ for each $p\in\mathbb{N}$
and each $m\times p$-matrix $A$.

\textbf{(c)} We have $A0_{n\times m}=0_{p\times m}$ for each $p\in\mathbb{N}$
and each $p\times n$-matrix $A$.

\textbf{(d)} We have $0A=0_{n\times m}$ for each $n\times m$-matrix $A$.
\end{proposition}

Numbers are known to be zero-divisor-free: If a product $ab$ of two numbers
$a$ and $b$ is $0$, then one of $a$ and $b$ must be $0$. This also fails for
matrices: If $A=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 0
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cc}%
0 & 0\\
0 & 1
\end{array}
\right)  $, then $AB=0_{2\times2}$ is the zero matrix, although neither $A$
nor $B$ is the zero matrix.

\subsection{The identity matrix}

\begin{definition}
Let $n\in\mathbb{N}$. The \textit{diagonal entries} of an $n\times n$-matrix
$A$ are its entries $A_{1,1},A_{2,2},\ldots,A_{n,n}$. In other words, they are
the entries $A_{i,j}$ for $i=j$.
\end{definition}

For example, the diagonal entries of $\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $ are $a$ and $d$. The name \textquotedblleft diagonal
entries\textquotedblright\ comes from the visualization of an $n\times
n$-matrix as a square table: When we say \textquotedblleft
diagonal\textquotedblright, we always mean the diagonal of the square that
connects the upper-left corner with the lower-right corner; the diagonal
entries are simply the entries along this diagonal. (The other diagonal is
called the \textquotedblleft antidiagonal\textquotedblright\ in linear algebra.)

\begin{definition}
Let $n\in\mathbb{N}$. Then, the $n\times n$ \textit{identity matrix} means the
matrix $\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, where%
\begin{equation}
\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for all }i\text{ and }j. \label{eq.def.In.eq}%
\end{equation}
This is the $n\times n$-matrix whose diagonal entries all equal $1$, and whose
all other entries equal $0$. It is denoted by $I_{n}$. (Other people call it
$I$ or $E$ or $E_{n}$.)
\end{definition}

For example, the $3\times3$ identity matrix is $I_{3}=\left(
\begin{array}
[c]{ccc}%
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right)  $.

The notation $\delta_{i,j}$ defined in (\ref{eq.def.In.eq}) is called the
\textit{Kronecker delta}; it is extremely simple and yet highly useful.

The identity matrix behaves very much like the number $1$:

\begin{proposition}
\label{prop.matrix-laws.id}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.

\textbf{(a)} We have $I_{n}A=A$ for each $n\times m$-matrix $A$.

\textbf{(b)} We have $AI_{m}=A$ for each $n\times m$-matrix $A$.
\end{proposition}

Proposition \ref{prop.matrix-laws.id} says that multiplying a matrix $A$ by an
identity matrix (from either side) does not change $A$. Thus, identity
matrices have no effect inside a product, and so can be \textquotedblleft
cancelled\textquotedblright\ (or, more precisely, dropped). For example, if
$A$, $B$, $C$ and $D$ are four $n\times n$-matrices, then $I_{n}ABI_{n}%
I_{n}CI_{n}D=ABCD$. (Of course, this is similar to dropping $1$'s from
products of numbers: $1ab\cdot1\cdot1c\cdot1d=abcd$.)

\subsection{(*) Proof of $AI_{n}=A$}

Let me give a proof of Proposition \ref{prop.matrix-laws.id} \textbf{(b)}, to
illustrate the following simple, yet important point about summations and the
Kronecker delta\footnote{This is something that often comes up in computations
(particularly in physics and computer science).}:

\begin{proposition}
\label{prop.sum.delta}Let $p$ and $q$ be two integers such that $p\leq q$. Let
$r\in\left\{  p,p+1,\ldots,q\right\}  $. Let $a_{p},a_{p+1},\ldots,a_{q}$ be
some numbers. Then,%
\[
\sum_{k=p}^{q}a_{k}\delta_{k,r}=a_{r}.
\]

\end{proposition}

\begin{example}
For $p=1$, $q=5$ and $r=4$, Proposition \ref{prop.sum.delta} says that
$\sum_{k=1}^{5}a_{k}\delta_{k,4}=a_{4}$. This is easy to check:%
\begin{align*}
\sum_{k=1}^{5}a_{k}\delta_{k,4}  &  =a_{1}\underbrace{\delta_{1,4}%
}_{\substack{=0\\\text{(since }1\neq4\text{)}}}+a_{2}\underbrace{\delta_{2,4}%
}_{\substack{=0\\\text{(since }2\neq4\text{)}}}+a_{3}\underbrace{\delta_{3,4}%
}_{\substack{=0\\\text{(since }3\neq4\text{)}}}+a_{4}\underbrace{\delta_{4,4}%
}_{\substack{=1\\\text{(since }4=4\text{)}}}+a_{5}\underbrace{\delta_{5,4}%
}_{\substack{=0\\\text{(since }5\neq4\text{)}}}\\
&  =a_{1}0+a_{2}0+a_{3}0+a_{4}1+a_{5}0=a_{4}1=a_{4}.
\end{align*}
What you should see on this example is that all but one addends of the sum
$\sum_{k=p}^{q}a_{k}\delta_{k,r}$ are zero, and the remaining one addend is
$a_{r}\underbrace{\delta_{r,r}}_{=1}=a_{r}1=a_{r}$. The proof below is just
writing this down in the general situation.
\end{example}

\begin{proof}
[Proof of Proposition \ref{prop.sum.delta}.]Let us first notice something
simple: For any $k\in\left\{  p,p+1,\ldots,q\right\}  $ such that $k\neq r$,
we have%
\[
a_{k}\underbrace{\delta_{k,r}}_{\substack{=0\\\text{(since }k\neq r\text{)}%
}}=a_{k}0=0.
\]
In other words, all terms of the form $a_{k}\delta_{k,r}$ with $k\neq r$ are
$0$. Hence, the sum of all these terms is $0$ as well. In other words,%
\begin{equation}
\left(  \text{the sum of all terms of the form }a_{k}\delta_{k,r}\text{ with
}k\neq r\right)  =0. \label{pf.prop.sum.delta.1}%
\end{equation}


By the definition of the $\sum$ sign, we have%
\begin{align*}
\sum_{k=p}^{q}a_{k}\delta_{k,r}  &  =a_{p}\delta_{p,r}+a_{p+1}\delta
_{p+1,r}+\cdots+a_{q}\delta_{q,r}\\
&  =\left(  \text{the sum of all terms of the form }a_{k}\delta_{k,r}\right)
\\
&  =a_{r}\underbrace{\delta_{r,r}}_{\substack{=1\\\text{(since }r=r\text{)}%
}}+\underbrace{\left(  \text{the sum of all terms of the form }a_{k}%
\delta_{k,r}\text{ with }k\neq r\right)  }_{\substack{=0\\\text{(by
(\ref{pf.prop.sum.delta.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have pulled out the addend
}a_{r}\delta_{r,r}\text{ out of the sum}\right) \\
&  =a_{r}1+0=a_{r}.
\end{align*}

\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.matrix-laws.id} \textbf{(b)}.]We have
$I_{m}=\left(  \delta_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$ (this is
how we defined $I_{m}$), and thus%
\begin{equation}
\left(  I_{m}\right)  _{u,v}=\delta_{u,v}\ \ \ \ \ \ \ \ \ \ \text{for all
}u\in\left\{  1,2,\ldots,m\right\}  \text{ and }v\in\left\{  1,2,\ldots
,m\right\}  . \label{pf.prop.matrix-laws.id.b.Im}%
\end{equation}


Let $A$ be an $n\times m$-matrix. For every $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}  $, we have%
\begin{align*}
\left(  AI_{m}\right)  _{i,j}  &  =\sum_{k=1}^{m}A_{i,k}\underbrace{\left(
I_{m}\right)  _{k,j}}_{\substack{=\delta_{k,j}\\\text{(by
(\ref{pf.prop.matrix-laws.id.b.Im}))}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
Proposition \ref{prop.AB}, applied to }p=m\text{ and }B=I_{m}\right) \\
&  =\sum_{k=1}^{m}A_{i,k}\delta_{k,j}=A_{i,j}%
\end{align*}
(by Proposition \ref{prop.sum.delta}, applied to $p=1$, $q=m$, $r=j$ and
$a_{k}=A_{i,k}$). In other words, each entry of the $n\times m$-matrix
$AI_{m}$ equals the corresponding entry of the $n\times m$-matrix $A$. In
other words, $AI_{m}$ equals $A$. This proves Proposition
\ref{prop.matrix-laws.id} \textbf{(b)}.
\end{proof}

Proposition \ref{prop.matrix-laws.id} \textbf{(a)} can be proven similarly
(but this time, instead of the sum $\sum_{k=1}^{m}A_{i,k}\delta_{k,j}$, we
must consider the sum $\sum_{k=1}^{m}\delta_{i,k}A_{k,j}=\sum_{k=1}^{m}%
A_{k,j}\delta_{i,k}=A_{i,j}$).

\subsection{Powers of a matrix}

The $k$-th power of a number $a$ (where $k\in\mathbb{N}$) is defined by
repeated multiplication: We start with $a^{0}=1$\ \ \ \ \footnote{Yes, this is
how $a^{0}$ is defined, for all $a$. Anyone who tells you that the number
$0^{0}$ is undefined is merely spreading their confusion.}, and we define each
next power of $a$ by multiplying the previous one by $a$. In formulas:
$a^{k+1}=a\cdot a^{k}$ for each $k\in\mathbb{N}$. Thus,%
\begin{align*}
a^{1}  &  =a\cdot\underbrace{a^{0}}_{=1}=a\cdot1=a;\\
a^{2}  &  =a\cdot\underbrace{a^{1}}_{=a}=a\cdot a;\\
a^{3}  &  =a\cdot\underbrace{a^{2}}_{=a\cdot a}=a\cdot a\cdot a,
\end{align*}
etc.. We can explicitly write
\[
a^{k}=\underbrace{a\cdot a\cdot\cdots\cdot a}_{k\text{ times }a}%
\ \ \ \ \ \ \ \ \ \ \text{for each }k\in\mathbb{N},
\]
where we understand $\underbrace{a\cdot a\cdot\cdots\cdot a}_{0\text{ times
}a}$ to mean $1$\ \ \ \ \footnote{This is a standard convention: An empty
product of numbers always means $1$.}.

We can play the same game with square matrices, but instead of the number $1$
we now take the $n\times n$ identity matrix $I_{n}$:

\begin{definition}
Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Then, the $k$-th power
of the matrix $A$ (where $k\in\mathbb{N}$) is defined by repeated
multiplication: We start with $A^{0}=I_{n}$, and we define each next power of
$A$ by multiplying the previous one by $A$. In formulas: $A^{k+1}=A\cdot
A^{k}$ for each $k\in\mathbb{N}$. Explicitly, $A^{k}=\underbrace{A\cdot
A\cdot\cdots\cdot A}_{k\text{ times }A}$ for each $k\in\mathbb{N}$, where the
empty product means $I_{n}$.
\end{definition}

Notice that we have been a bit sloppy when we said \textquotedblleft
multiplying the previous one by $A$\textquotedblright: When we multiply a
matrix $B$ by $A$, we might mean either $AB$ or $BA$, and as we know, these
two products can be different (matrices don't always commute!). However, in
the above definition, this makes no matter, because both definitions lead to
the same explicit formula $A^{k}=\underbrace{A\cdot A\cdot\cdots\cdot
A}_{k\text{ times }A}$ (which is well-defined because of general associativity).

We now have a first moderately interesting example of commuting matrices: Any
two powers of a square matrix commute. (In other words: For any $n\times
n$-matrix $A$, and any $u\in\mathbb{N}$ and $v\in\mathbb{N}$, the two matrices
$A^{u}$ and $A^{v}$ commute. This follows by observing that $A^{u}%
A^{v}=A^{u+v}=A^{v}A^{u}$.)

\subsection{(*) Application: Fibonacci numbers}

Here is a simple application of matrix multiplication to elementary mathematics.

\begin{definition}
\label{def.fibonacci}The \textit{Fibonacci sequence} is the sequence $\left(
0,1,1,2,3,5,8,13,21,34,55,\ldots\right)  $ that is defined as follows: Its
first two entries are $0$ and $1$, and each further entry is the sum of the
previous entries. In more formal terms, it is the sequence $\left(
f_{0},f_{1},f_{2},f_{3},\ldots\right)  $ (we start the labelling at $0$)
defined recursively by%
\begin{align*}
f_{0}  &  =0,\ \ \ \ \ \ \ \ \ \ f_{1}=1,\ \ \ \ \ \ \ \ \ \ \text{and}\\
f_{n}  &  =f_{n-1}+f_{n-2}\ \ \ \ \ \ \ \ \ \ \text{for every }n\geq2.
\end{align*}
The elements of this sequence are called the \textit{Fibonacci numbers}.
\end{definition}

Definition \ref{def.fibonacci} gives a straightforward way to compute each
given Fibonacci number $f_{n}$, by computing the first $n+1$ Fibonacci numbers
$f_{0},f_{1},\ldots,f_{n}$ one after the others. However, when $n$ is large,
this takes some time (each of the $n+1$ first Fibonacci numbers has to be
computed!). Is there a faster way to compute Fibonacci numbers?

It turns out that there is. It is based on the following fact:

\begin{proposition}
\label{prop.fibonacci.matrix}Let $B$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  $. Then, for every positive integer $n$, we have%
\begin{equation}
B^{n}=\left(
\begin{array}
[c]{cc}%
f_{n+1} & f_{n}\\
f_{n} & f_{n-1}%
\end{array}
\right)  . \label{eq.prop.fibonacci.matrix.1}%
\end{equation}

\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.fibonacci.matrix}.]We shall prove Proposition
\ref{prop.fibonacci.matrix} by induction over $n$:

\textit{Induction base:} We have $B^{1}=B=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  $. Comparing this with%
\[
\left(
\begin{array}
[c]{cc}%
f_{1+1} & f_{1}\\
f_{1} & f_{1-1}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }f_{1+1}=f_{2}=1\text{,
}f_{1}=1\text{ and }f_{1-1}=f_{0}=0\right)  ,
\]
we obtain $B^{1}=\left(
\begin{array}
[c]{cc}%
f_{1+1} & f_{1}\\
f_{1} & f_{1-1}%
\end{array}
\right)  $. In other words, Proposition \ref{prop.fibonacci.matrix} holds for
$n=1$. This completes the induction base. (This was a completely
straightforward computation, and in the future will be left to the reader.)

\textit{Induction step:} Let $N$ be a positive integer. Assume that
Proposition \ref{prop.fibonacci.matrix} holds for $n=N$. We must show that
Proposition \ref{prop.fibonacci.matrix} also holds for $n=N+1$.

The definition of the Fibonacci sequence shows that $f_{N+2}=f_{N+1}+f_{N}$
and $f_{N+1}=f_{N}+f_{N-1}$.

We have assumed that Proposition \ref{prop.fibonacci.matrix} holds for $n=N$.
In other words,%
\[
B^{N}=\left(
\begin{array}
[c]{cc}%
f_{N+1} & f_{N}\\
f_{N} & f_{N-1}%
\end{array}
\right)  .
\]
Now,
\begin{align*}
B^{N+1}  &  =\underbrace{B^{N}}_{=\left(
\begin{array}
[c]{cc}%
f_{N+1} & f_{N}\\
f_{N} & f_{N-1}%
\end{array}
\right)  }\underbrace{B}_{=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  }=\left(
\begin{array}
[c]{cc}%
f_{N+1} & f_{N}\\
f_{N} & f_{N-1}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
f_{N+1}\cdot1+f_{N}\cdot1 & f_{N+1}\cdot1+f_{N}\cdot0\\
f_{N}\cdot1+f_{N-1}\cdot1 & f_{N}\cdot1+f_{N-1}\cdot0
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of a product of two
matrices}\right) \\
&  =\left(
\begin{array}
[c]{cc}%
f_{N+1}+f_{N} & f_{N+1}\\
f_{N}+f_{N-1} & f_{N}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
f_{N+2} & f_{N+1}\\
f_{N+1} & f_{N}%
\end{array}
\right)
\end{align*}
(since $f_{N+1}+f_{N}=f_{N+2}$ and $f_{N}+f_{N-1}=f_{N+1}$). In other words,
Proposition \ref{prop.fibonacci.matrix} holds for $n=N+1$. This completes the
induction step; hence, Proposition \ref{prop.fibonacci.matrix} is proven.
\end{proof}

How does Proposition \ref{prop.fibonacci.matrix} help us compute $f_{n}$
quickly? Naively computing $B^{n}$ by multiplying $B$ with itself $n$ times is
not any faster than computing $f_{n}$ directly using Definition
\ref{def.fibonacci} (in fact, it is slower, since multiplying matrices takes
longer than adding numbers). However, there is a trick for computing powers
quickly, called \textit{binary exponentiation}; this trick works just as well
for matrices as it does for numbers. The trick uses the following observations:

\begin{itemize}
\item For every $m\in\mathbb{N}$, we have $B^{2m}=\left(  B^{m}\right)  ^{2}$.

\item For every $m\in\mathbb{N}$, we have $B^{2m+1}=B\left(  B^{m}\right)
^{2}$.
\end{itemize}

These observations allow us to quickly compute $B^{2m}$ and $B^{2m+1}$ using
only $B^{m}$; thus, we can \textquotedblleft jump up\textquotedblright\ from
$B^{m}$ directly to $B^{2m}$ and to $B^{2m+1}$ without the intermediate steps
$B^{m+1},B^{m+2},\ldots,B^{2m-1}$. Let us use this to compute $B^{90}$ (and
thus $f_{90}$) quickly (without computing $91$ Fibonacci numbers):

\begin{itemize}
\item We want to find $B^{90}$. Since $90=2\cdot45$, we have $B^{90}=\left(
B^{45}\right)  ^{2}$ (by the formula $B^{2m}=\left(  B^{m}\right)  ^{2}$).

\item We thus want to find $B^{45}$. Since $45=2\cdot22+1$, we have
$B^{45}=B\left(  B^{22}\right)  ^{2}$ (by the formula $B^{2m+1}=B\left(
B^{m}\right)  ^{2}$).

\item We thus want to find $B^{22}$. Since $22=2\cdot11$, we have
$B^{22}=\left(  B^{11}\right)  ^{2}$ (by the formula $B^{2m}=\left(
B^{m}\right)  ^{2}$).

\item We thus want to find $B^{11}$. Since $11=2\cdot5+1$, we have
$B^{11}=B\left(  B^{5}\right)  ^{2}$ (by the formula $B^{2m+1}=B\left(
B^{m}\right)  ^{2}$).

\item We thus want to find $B^{5}$. Since $5=2\cdot2+1$, we have
$B^{5}=B\left(  B^{2}\right)  ^{2}$ (by the formula $B^{2m+1}=B\left(
B^{m}\right)  ^{2}$).

\item We thus want to find $B^{2}$. Since $2=2\cdot1$, we have $B^{2}=\left(
B^{1}\right)  ^{2}$ (by the formula $B^{2m}=\left(  B^{m}\right)  ^{2}$, but
this was obvious anyway).
\end{itemize}

We know what $B^{1}$ is: $B^{1}=B=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  $. Hence, $B^{2}=\left(  B^{1}\right)  ^{2}$ becomes $B^{2}=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  ^{2}=\left(
\begin{array}
[c]{cc}%
2 & 1\\
1 & 1
\end{array}
\right)  $. Hence, $B^{5}=B\left(  B^{2}\right)  ^{2}$ becomes $B^{5}=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
2 & 1\\
1 & 1
\end{array}
\right)  ^{2}=\left(
\begin{array}
[c]{cc}%
8 & 5\\
5 & 3
\end{array}
\right)  $. Hence, $B^{11}=B\left(  B^{5}\right)  ^{2}$ becomes $B^{11}%
=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
8 & 5\\
5 & 3
\end{array}
\right)  ^{2}=\left(
\begin{array}
[c]{cc}%
144 & 89\\
89 & 55
\end{array}
\right)  $. Hence, $B^{22}=\left(  B^{11}\right)  ^{2}$ becomes $B^{22}%
=\left(
\begin{array}
[c]{cc}%
144 & 89\\
89 & 55
\end{array}
\right)  ^{2}=\left(
\begin{array}
[c]{cc}%
28657 & 17711\\
17711 & 10946
\end{array}
\right)  $. Hence, $B^{45}=B\left(  B^{22}\right)  ^{2}$ becomes
\newline$B^{45}=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
28657 & 17711\\
17711 & 10946
\end{array}
\right)  ^{2}=\left(
\begin{array}
[c]{cc}%
1836311903 & 1134903170\\
1134903170 & 701408733
\end{array}
\right)  $. Hence, $B^{90}=\left(  B^{45}\right)  ^{2}$ becomes \newline%
$B^{90}=\left(
\begin{array}
[c]{cc}%
1836311903 & 1134903170\\
1134903170 & 701408733
\end{array}
\right)  ^{2}=\allowbreak\left(
\begin{array}
[c]{cc}%
4660046610375530309 & 2880067194370816120\\
2880067194370816120 & 1779979416004714189
\end{array}
\right)  $. Since $f_{90}$ is the $\left(  2,1\right)  $-th entry of $B^{90}$
(indeed, (\ref{eq.prop.fibonacci.matrix.1}) shows that $f_{n}$ is the $\left(
2,1\right)  $-th entry of $B^{n}$ for all positive integers $n$), we thus
obtain%
\[
f_{90}=2880067194370816120.
\]


\begin{exercise}
\label{exe.fibonacci.Fn+m}Show that, for any two positive integers $n$ and
$m$, we have%
\[
f_{n+m}=f_{n}f_{m+1}+f_{n-1}f_{m}.
\]


[\textbf{Hint:} Begin with the equality $B^{n}B^{m}=B^{n+m}$. Rewrite it using
Proposition \ref{prop.fibonacci.matrix}, and compare entries.]
\end{exercise}

\section{Gaussian elimination}

\subsection{Linear equations and matrices}

We now take aim at understanding Gaussian elimination in terms of matrices.
First of all, let us see what solving linear equations has to do with matrices.

\begin{example}
\label{exam.systems}Consider the following system of equations in three
unknowns $x,y,z$:%
\begin{equation}
\left\{
\begin{array}
[c]{c}%
3x+6y-z=2;\\
7x+4y-3z=3;\\
-y+8z=1
\end{array}
\right.  . \label{eq.exam.systems.1}%
\end{equation}
I claim that this system of equations is equivalent to the single equation%
\begin{equation}
\left(
\begin{array}
[c]{ccc}%
3 & 6 & -1\\
7 & 4 & -3\\
0 & -1 & 8
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
x\\
y\\
z
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
2\\
3\\
1
\end{array}
\right)  \label{eq.exam.systems.2}%
\end{equation}
(this is an equation between two column vectors of size $3$, so no wonder that
it encodes a whole system of linear equations).

Why are (\ref{eq.exam.systems.1}) and (\ref{eq.exam.systems.2}) equivalent?
Well, the left hand side of (\ref{eq.exam.systems.2}) is%
\[
\left(
\begin{array}
[c]{ccc}%
3 & 6 & -1\\
7 & 4 & -3\\
0 & -1 & 8
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
x\\
y\\
z
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
3x+6y+\left(  -1\right)  z\\
7x+4y+\left(  -3\right)  z\\
0x+\left(  -1\right)  y+8z
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
3x+6y-z\\
7x+4y-3z\\
-y+8z
\end{array}
\right)  .
\]
Thus, (\ref{eq.exam.systems.2}) is equivalent to%
\[
\left(
\begin{array}
[c]{c}%
3x+6y-z\\
7x+4y-3z\\
-y+8z
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
2\\
3\\
1
\end{array}
\right)  .
\]
But this is clearly equivalent to (\ref{eq.exam.systems.1}).
\end{example}

More generally, the system of $m$ linear equations
\[
\left\{
\begin{array}
[c]{c}%
a_{1,1}x_{1}+a_{1,2}x_{2}+\cdots+a_{1,n}x_{n}=b_{1};\\
a_{2,1}x_{1}+a_{2,2}x_{2}+\cdots+a_{2,n}x_{n}=b_{2};\\
\vdots\\
a_{m,1}x_{1}+a_{m,2}x_{2}+\cdots+a_{m,n}x_{n}=b_{m}%
\end{array}
\right.
\]
in $n$ unknowns $x_{1},x_{2},\ldots,x_{n}$ is equivalent to the vector
equation%
\begin{equation}
\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,n}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m,1} & a_{m,2} & \cdots & a_{m,n}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
x_{1}\\
x_{2}\\
\vdots\\
x_{n}%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
b_{1}\\
b_{2}\\
\vdots\\
b_{m}%
\end{array}
\right)  . \label{eq.exam.systems.gen}%
\end{equation}
In other words, it is equivalent to the vector equation%
\begin{equation}
Ax=b, \label{eq.exam.systems.gen.short}%
\end{equation}
where%
\begin{align*}
A  &  =\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,n}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m,1} & a_{m,2} & \cdots & a_{m,n}%
\end{array}
\right)  ,\\
x  &  =\left(
\begin{array}
[c]{c}%
x_{1}\\
x_{2}\\
\vdots\\
x_{n}%
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ b=\left(
\begin{array}
[c]{c}%
b_{1}\\
b_{2}\\
\vdots\\
b_{m}%
\end{array}
\right)  .
\end{align*}
(Some authors write the $A$, the $x$ and the $b$ in
(\ref{eq.exam.systems.gen.short}) in boldface in order to stress that these
are vectors, not numbers. We shall not.) The matrix $A$ and the vector $b$ are
known; the vector $x$ is what we want to find.

Thus, matrices give us a way to rewrite systems of linear equations as single
equations between vectors. Moreover, as we will see, they give us a way to
manipulate these equations easily.

To solve a vector equation like (\ref{eq.exam.systems.gen.short}) means (in
some sense) to \textquotedblleft undo\textquotedblright\ a matrix
multiplication. In fact, if we could divide by a matrix, then we could
immediately solve $Ax=b$ by \textquotedblleft dividing by $A$%
\textquotedblright. Unfortunately, we cannot divide by a matrix in general.
But the idea is fruitful: In fact, some matrices $A$ are invertible (i.e.,
have an inverse $A^{-1}$), and for those matrices, we can transform $Ax=b$
into $x=A^{-1}b$, which gives us an explicit and unique solution for the
system (\ref{eq.exam.systems.gen}). This doesn't work for all $A$ (since not
all $A$ are invertible), and is not a very practical way of solving systems of
linear equations; but the notion of invertible matrices is rather important,
so we begin by studying them.

\subsection{Inverse matrices}

\begin{definition}
\label{def.inverse-matrix}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times m$-matrix, and $B$ be an $m\times n$-matrix.

\textbf{(a)} We say that $B$ is a \textit{right inverse} of $A$ if $AB=I_{n}$.

\textbf{(b)} We say that $B$ is a \textit{left inverse} of $A$ if $BA=I_{m}$.

\textbf{(c)} We say that $B$ is an \textit{inverse} of $A$ if both $AB=I_{n}$
and $BA=I_{m}$.
\end{definition}

Notice that we are saying \textquotedblleft\textbf{a} right
inverse\textquotedblright\ (not \textquotedblleft\textbf{the} right
inverse\textquotedblright) in Definition \ref{def.inverse-matrix}, because a
given matrix $A$ can have several right inverses (but it can also have no
right inverses at all). For the same reason, we are saying \textquotedblleft%
\textbf{a} left inverse\textquotedblright\ (not \textquotedblleft\textbf{the}
left inverse\textquotedblright). However, when we are saying \textquotedblleft%
\textbf{an} inverse\textquotedblright\ (not \textquotedblleft\textbf{the}
inverse\textquotedblright), we are just being cautious: We will later (in
Corollary \ref{cor.inverse.unique}) see that $A$ can never have several
different inverses; thus, it would be legitimate to say \textquotedblleft%
\textbf{the} inverse\textquotedblright\ as well. But as long as we have not
proven this, we shall speak of \textquotedblleft\textbf{an}
inverse\textquotedblright.

\begin{example}
\label{exam.inverses}\textbf{(a)} Let $A=\left(  1,4\right)  $. (Recall that
this means the $1\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & 4
\end{array}
\right)  $.) When is a matrix $B$ a right inverse of $A$ ?

First, if $B$ is a right inverse of $A$, then $B$ must be a $2\times1$-matrix
(since any right inverse of an $n\times m$-matrix has to be an $m\times
n$-matrix). So let us assume that $B$ is a $2\times1$-matrix. Thus, $B$ must
have the form $B=\left(
\begin{array}
[c]{c}%
u\\
v
\end{array}
\right)  $ for some numbers $u$ and $v$. Then, $AB=\left(
\begin{array}
[c]{cc}%
1 & 4
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
u\\
v
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1u+4v
\end{array}
\right)  $. In order for $B$ to be a right inverse of $A$, it is necessary and
sufficient that $AB=I_{1}$ (because this is how we defined \textquotedblleft
right inverse\textquotedblright). In other words, we must have $\left(
\begin{array}
[c]{c}%
1u+4v
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1
\end{array}
\right)  $ (since $AB=\left(
\begin{array}
[c]{c}%
1u+4v
\end{array}
\right)  $ and $I_{1}=\left(
\begin{array}
[c]{c}%
1
\end{array}
\right)  $). In other words, we must have $1u+4v=1$.

Hence, a matrix $B$ is a right inverse of $A$ if and only if it has the form
$B=\left(
\begin{array}
[c]{c}%
u\\
v
\end{array}
\right)  $ for some numbers $u$ and $v$ satisfying $1u+4v=1$. How do we find
two such numbers $u$ and $v$ ? Well, we can view $1u+4v=1$ as a system of $1$
linear equation in $2$ variables, but actually we can just read off the
solution: $v$ can be chosen arbitrarily, and $u$ then has to be $1-4v$. Hence,
a matrix $B$ is a right inverse of $A$ if and only if it has the form
$B=\left(
\begin{array}
[c]{c}%
1-4v\\
v
\end{array}
\right)  $ for some number $v$. In particular, there are \textbf{infinitely
many} matrices $B$ that are right inverses of $A$ (because we have full
freedom in choosing $v$).

\textbf{(b)} Let $A=\left(  1,4\right)  $ again. When is a matrix $B$ a left
inverse of $A$ ?

Again, $B$ must be a $2\times1$-matrix in order for this to have any chance of
being true. So let us assume that $B$ is a $2\times1$-matrix, and write $B$ in
the form $B=\left(
\begin{array}
[c]{c}%
u\\
v
\end{array}
\right)  $ for some numbers $u$ and $v$. Then, $BA=\left(
\begin{array}
[c]{c}%
u\\
v
\end{array}
\right)  \left(  1,4\right)  =\left(
\begin{array}
[c]{cc}%
u\cdot1 & u\cdot4\\
v\cdot1 & v\cdot4
\end{array}
\right)  $. In order for $B$ to be a left inverse of $A$, it is necessary and
sufficient that $BA=I_{2}$ (because this is how we defined \textquotedblleft
left inverse\textquotedblright). In other words, we must have $\left(
\begin{array}
[c]{cc}%
u\cdot1 & u\cdot4\\
v\cdot1 & v\cdot4
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  $ (since $BA=\left(
\begin{array}
[c]{cc}%
u\cdot1 & u\cdot4\\
v\cdot1 & v\cdot4
\end{array}
\right)  $ and $I_{2}=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  $). In other words, we must have%
\[
\left\{
\begin{array}
[c]{c}%
u\cdot1=1;\\
u\cdot4=0;\\
v\cdot1=0;\\
v\cdot4=1
\end{array}
\right.  .
\]
But this cannot happen! Indeed, the equations $u\cdot1=1$ and $u\cdot4=0$
contradict each other (because if $u\cdot1=1$, then $u\cdot4$ must be $4$).
Hence, $B$ can never be a left inverse of $A$. In other words, the matrix $A$
\textbf{has no} left inverse.

\textbf{(c)} Let $A=\left(
\begin{array}
[c]{c}%
1\\
4
\end{array}
\right)  $. When is a matrix $B$ a left inverse of $A$ ? When is a matrix $B$
a right inverse of $A$ ? I will let you figure this out (see Exercise
\ref{exe.exam.inverses} below).

\textbf{(d)} Let $A=\left(
\begin{array}
[c]{cc}%
1 & -1\\
1 & 1
\end{array}
\right)  $. When is a matrix $B$ a left inverse of $A$ ? When is a matrix $B$
a right inverse of $A$ ?

A left inverse of $A$ would have to be a $2\times2$-matrix. A $2\times
2$-matrix $B=\left(
\begin{array}
[c]{cc}%
x & y\\
z & w
\end{array}
\right)  $ is a left inverse of $A$ if and only if it satisfies $BA=I_{2}$,
that is, $\left(
\begin{array}
[c]{cc}%
x & y\\
z & w
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
1 & -1\\
1 & 1
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  $, or, equivalently, $\left(
\begin{array}
[c]{cc}%
x+y & -x+y\\
w+z & w-z
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  $, or, equivalently,%
\[
\left\{
\begin{array}
[c]{c}%
x+y=1;\\
-x+y=0;\\
w+z=0;\\
w-z=1
\end{array}
\right.  .
\]
This is a system of four linear equations in the four unknowns $x,y,z,w$; it
has the unique solution%
\[
\left(  x,y,z,w\right)  =\left(  \dfrac{1}{2},\dfrac{1}{2},-\dfrac{1}%
{2},\dfrac{1}{2}\right)  .
\]
Thus, a $2\times2$-matrix $B=\left(
\begin{array}
[c]{cc}%
x & y\\
z & w
\end{array}
\right)  $ is a left inverse of $A$ if and only if $\left(  x,y,z,w\right)
=\left(  \dfrac{1}{2},\dfrac{1}{2},-\dfrac{1}{2},\dfrac{1}{2}\right)  $.
Hence, there exists exactly one left inverse of $A$, and this left inverse is
$\left(
\begin{array}
[c]{cc}%
\dfrac{1}{2} & \dfrac{1}{2}\\
-\dfrac{1}{2} & \dfrac{1}{2}%
\end{array}
\right)  $.

A similar computation reveals that there exists exactly one right inverse of
$A$, and this right inverse is $\left(
\begin{array}
[c]{cc}%
\dfrac{1}{2} & \dfrac{1}{2}\\
-\dfrac{1}{2} & \dfrac{1}{2}%
\end{array}
\right)  $. So the unique left inverse of $A$ and the unique right inverse of
$A$ are actually equal (and thus is an inverse of $A$). This might not be
clear from the definitions, but as we shall soon see, this is not a coincidence.

\textbf{(e)} Generalizing Example \ref{exam.inverses} \textbf{(d)}, we might
wonder when a $2\times2$-matrix $A=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $ has a left inverse, a right inverse or an inverse. For any given
four values of $a,b,c,d$, we can answer this question similarly how we
answered it for the matrix $A=\left(
\begin{array}
[c]{cc}%
1 & -1\\
1 & 1
\end{array}
\right)  $ in Example \ref{exam.inverses} \textbf{(d)} (by solving a system of
linear equation). The procedure will depend on whether some numbers are zero
or not. (For example, we might want to divide the equation $bx+dy=0$ by $b$,
which requires $b\neq0$; the case $b=0$ will then have to be treated
separately.) But the final result will be the following:

\begin{itemize}
\item If $ad-bc=0$, then the matrix $A$ has no left inverses and no right inverses.

\item If $ad-bc\neq0$, then the matrix $A$ has a unique inverse, which is also
the unique left inverse and the unique right inverse. This inverse is $\left(
%
\begin{array}
[c]{cc}%
\dfrac{d}{ad-bc} & -\dfrac{b}{ad-bc}\\
-\dfrac{c}{ad-bc} & \dfrac{a}{ad-bc}%
\end{array}
\right)  $.
\end{itemize}

Again, this phenomenon of the left inverse equalling the right inverse
appears. Notably, the number $ad-bc$ plays an important role here; we will
later see more of it (it is an example of a \textit{determinant}).
\end{example}

\begin{exercise}
\label{exe.exam.inverses}Let $A=\left(
\begin{array}
[c]{c}%
1\\
4
\end{array}
\right)  $. When is a matrix $B$ a left inverse of $A$ ? When is a matrix $B$
a right inverse of $A$ ?
\end{exercise}

As we know from Example \ref{exam.inverses} \textbf{(a)}, a matrix may have
infinitely many right inverses. Similarly, a matrix may have infinitely many
left inverses. But can a matrix have both infinitely many right inverses and
infinitely many left inverses at the same time? The answer is
\textquotedblleft no\textquotedblright, and in fact, something stronger is true:

\begin{proposition}
\label{prop.inverses.L=R}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ be
an $n\times m$-matrix. Let $L$ be a left inverse of $A$. Let $R$ be a right
inverse of $A$. Then:

\textbf{(a)} We have $L=R$.

\textbf{(b)} The matrix $L$ is the only left inverse of $A$.

\textbf{(c)} The matrix $R$ is the only right inverse of $A$.

\textbf{(d)} The matrix $L=R$ is the only inverse of $A$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.inverses.L=R}.]We know that $LA=I_{m}$ (since
$L$ is a left inverse of $A$) and that $AR=I_{n}$ (since $R$ is a right
inverse of $A$).

\textbf{(a)} Consider the product $LAR$. (Recall that this product is
well-defined, because Proposition \ref{prop.matrix-laws.1} \textbf{(g)} yields
$L\left(  AR\right)  =\left(  LA\right)  R$.)

One way to rewrite $LAR$ is as follows:%
\begin{equation}
L\underbrace{AR}_{=I_{n}}=LI_{n}=L\ \ \ \ \ \ \ \ \ \ \left(  \text{by
Proposition \ref{prop.matrix-laws.id} \textbf{(b)}}\right)  .
\label{pf.prop.inverses.L=R.1}%
\end{equation}
Another way is%
\begin{equation}
\underbrace{LA}_{=I_{m}}R=I_{m}R=R\ \ \ \ \ \ \ \ \ \ \left(  \text{by
Proposition \ref{prop.matrix-laws.id} \textbf{(a)}}\right)  .
\label{pf.prop.inverses.L=R.2}%
\end{equation}
Comparing (\ref{pf.prop.inverses.L=R.1}) with (\ref{pf.prop.inverses.L=R.2}),
we obtain $L=R$. This proves Proposition \ref{prop.inverses.L=R} \textbf{(a)}.

\textbf{(b)} Let $L^{\prime}$ be any left inverse of $A$. Then, we can apply
Proposition \ref{prop.inverses.L=R} \textbf{(a)} to $L^{\prime}$ instead of
$L$ (because all that was needed from $L$ in Proposition
\ref{prop.inverses.L=R} \textbf{(a)} was that it be a left inverse of $A$). As
a result, we obtain $L^{\prime}=R$.

Now, forget that we fixed $L^{\prime}$. We thus have shown that if $L^{\prime
}$ is any left inverse of $A$, then $L^{\prime}=R$. In other words, any left
inverse of $A$ equals $R$. Thus, there exists at most one left inverse of $A$.
Therefore, the matrix $L$ is the only left inverse of $A$. This proves
Proposition \ref{prop.inverses.L=R} \textbf{(b)}.

\textbf{(c)} The proof of Proposition \ref{prop.inverses.L=R} \textbf{(c)} is
analogous to the proof of Proposition \ref{prop.inverses.L=R} \textbf{(b)}.
(We again need to apply Proposition \ref{prop.inverses.L=R} \textbf{(a)}
twice, but this time, instead of a left inverse $L^{\prime}$, we have to
introduce a right inverse $R^{\prime}$. The details are left to the reader.)

\textbf{(d)} Proposition \ref{prop.inverses.L=R} \textbf{(a)} yields $L=R$.
Hence, $A\underbrace{L}_{=R}=AR=I_{n}$.

Now, the matrix $L$ is an inverse of $A$ (since $LA=I_{m}$ and $AL=I_{n}$). In
other words, the matrix $L=R$ is an inverse of $A$ (since $L=R$). It remains
to show that it is the only inverse of $A$. But this is easy: Let $L^{\prime}$
be any inverse of $A$. Then, $L^{\prime}A=I_{m}$, so that $L^{\prime}$ is a
left inverse of $A$. Proposition \ref{prop.inverses.L=R} \textbf{(a)} (applied
to $L^{\prime}$ instead of $L$) therefore yields $L^{\prime}=R$.

Now, forget that we fixed $L^{\prime}$. We thus have shown that if $L^{\prime
}$ is any inverse of $A$, then $L^{\prime}=R$. In other words, any inverse of
$A$ equals $R$. Thus, there exists at most one inverse of $A$. Therefore, the
matrix $L$ is the only inverse of $A$ (since we already know that $L$ is an
inverse of $A$). This proves Proposition \ref{prop.inverses.L=R} \textbf{(d)}.
\end{proof}

\begin{corollary}
\label{cor.inverses.unique}Let $A$ be a matrix. Then, $A$ has at most one inverse.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.inverses.unique}.]We need to show that any two
inverses of $A$ equal. So let $L$ and $R$ be two inverses of $A$. We must show
that $L=R$.

Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be such that $A$ is an $n\times
m$-matrix. The matrix $L$ is an inverse of $A$, thus satisfies $LA=I_{m}$.
Hence, $L$ is a left inverse of $A$. Also, the matrix $R$ is an inverse of
$A$, thus satisfies $AR=I_{n}$. Hence, $R$ is a right inverse of $A$. Thus,
Proposition \ref{prop.inverses.L=R} \textbf{(a)} shows that $L=R$. This proves
Corollary \ref{cor.inverses.unique}.
\end{proof}

\begin{definition}
\textbf{(a)} A matrix $A$ is said to be \textit{invertible} if it has an
inverse. (Similarly, we can define the words \textquotedblleft
left-invertible\textquotedblright\ and \textquotedblleft
right-invertible\textquotedblright.)

\textbf{(b)} Let $A$ be an invertible matrix. Then, $A$ has an inverse. Due to
Corollary \ref{cor.inverses.unique}, we furthermore know that $A$ has at most
one inverse. Thus, $A$ has exactly one inverse. We can thus call it
\textquotedblleft\textbf{the} inverse of $A$\textquotedblright, and denote it
by $A^{-1}$. If $A$ is an $n\times m$-matrix, then this inverse satisfies
$A^{-1}A=I_{m}$ and $AA^{-1}=I_{n}$ (by its definition).
\end{definition}

Notice that the equalities $A^{-1}A=I_{m}$ and $AA^{-1}=I_{n}$ show that a
matrix $A$ and its inverse $A^{-1}$ cancel each other when they stand adjacent
in a product: for example, $BA^{-1}AC$ simplifies to $BC$. However, they do
not (generally) cancel each other when they appear apart from one another: for
example, $BA^{-1}CA$ does \textbf{not} simplify to $BC$.

So what matrices are invertible? The following theorem significantly narrows
the search down; we shall not prove it until later:

\begin{theorem}
\label{thm.invertible.size}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times m$-matrix.

\textbf{(a)} If $A$ has a right inverse, then $n\leq m$ (that is, the matrix
$A$ has at least as many columns as it has rows).

\textbf{(b)} If $A$ has a left inverse, then $n\geq m$ (that is, the matrix
$A$ has at most as many columns as it has rows).

\textbf{(c)} If $A$ is invertible (i.e., has an inverse), then $n=m$ (that is,
the matrix $A$ is square).

\textbf{(d)} If $A$ is square (that is, $n=m$) and has a left inverse
\textbf{or} a right inverse, then $A$ is actually invertible (and so this left
or right inverse is the inverse of $A$). Notice that this is false for
rectangular matrices!
\end{theorem}

Let us now check some simpler facts about inverses:

\begin{proposition}
\label{prop.inverses.In}Let $n\in\mathbb{N}$. Then, the matrix $I_{n}$ is
invertible, and its inverse is $\left(  I_{n}\right)  ^{-1}=I_{n}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.inverses.In}.]We have $I_{n}I_{n}=I_{n}$ and
$I_{n}I_{n}=I_{n}$. Hence, the matrix $I_{n}$ is an inverse of $I_{n}$ (by the
definition of \textquotedblleft inverse\textquotedblright). This proves
Proposition \ref{prop.inverses.In}.
\end{proof}

\begin{proposition}
\label{prop.inverses.AB}Let $A$ and $B$ be two invertible matrices such that
the product $AB$ is well-defined (i.e., such that $A$ has as many columns as
$B$ has rows). Then, the matrix $AB$ is also invertible, and its inverse is
\begin{equation}
\left(  AB\right)  ^{-1}=B^{-1}A^{-1}. \label{eq.prop.inverses.AB.eq}%
\end{equation}

\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.inverses.AB}.]Let $n$, $m$ and $p$ be
nonnegative integers such that $A$ is an $n\times m$-matrix and $B$ is an
$m\times p$-matrix\footnote{We can indeed find such $n$, $m$ and $p$ because
$A$ has as many columns as $B$ has rows.}. (Actually, Theorem
\ref{thm.invertible.size} \textbf{(c)} reveals that the matrices $A$ and $B$
are square and therefore $n=m=p$; but I do not want to use Theorem
\ref{thm.invertible.size} \textbf{(c)} here, since I have not yet proven it.)

Recall once again that (by general associativity) products of matrices can be
written without parentheses. Thus, for example, the products $B^{-1}A^{-1}AB$
and $ABB^{-1}A^{-1}$ make sense. Let us simplify these products:%
\[
B^{-1}\underbrace{A^{-1}A}_{=I_{m}}B=B^{-1}I_{m}B=B^{-1}B=I_{p}%
\]
and%
\[
A\underbrace{BB^{-1}}_{=I_{m}}A^{-1}=AI_{m}A^{-1}=AA^{-1}=I_{n}.
\]
But these two equalities say precisely that $B^{-1}A^{-1}$ is an inverse of
$AB$. (If you don't believe me, rewrite them with parentheses: $\left(
B^{-1}A^{-1}\right)  \left(  AB\right)  =I_{p}$ and $\left(  AB\right)
\left(  B^{-1}A^{-1}\right)  =I_{n}$.) In particular, this shows that $AB$ is
invertible. This proves Proposition \ref{prop.inverses.AB}.
\end{proof}

In words, (\ref{eq.prop.inverses.AB.eq}) says that the inverse of a product of
two matrices is the product of their inverses, but \textbf{in opposite order}.
This takes some getting used to, but is really a natural thing; the same rule
holds for inverting the composition of functions\footnote{Namely: If $X$, $Y$
and $Z$ are three sets, and if $b:X\rightarrow Y$ and $a:Y\rightarrow Z$ are
two invertible functions (i.e., bijections), then $a\circ b:X\rightarrow Z$ is
an invertible function as well, and its inverse is $\left(  a\circ b\right)
^{-1}=b^{-1}\circ a^{-1}$. (Some authors liken this to the fact that if you
want to undo the process of putting on socks and then putting on shoes, you
have to first take off your shoes and then take off your socks. See
\url{https://proofwiki.org/wiki/Inverse_of_Product} .)}.

\begin{proposition}
\label{prop.inverses.A1Ak}Let $A_{1},A_{2},\ldots,A_{k}$ be $k$ invertible
matrices (where $k$ is a positive integer) such that the product $A_{1}%
A_{2}\cdots A_{k}$ is well-defined (i.e., such that $A_{i}$ has as many
columns as $A_{i+1}$ has rows, for each $i<k$). Then, the matrix $A_{1}%
A_{2}\cdots A_{k}$ is invertible, and its inverse is%
\[
\left(  A_{1}A_{2}\cdots A_{k}\right)  ^{-1}=A_{k}^{-1}A_{k-1}^{-1}\cdots
A_{1}^{-1}.
\]

\end{proposition}

Proposition \ref{prop.inverses.A1Ak} is a natural extension of Proposition
\ref{prop.inverses.AB} to products of more than $2$ matrices. The proof of
Proposition \ref{prop.inverses.A1Ak} is straightforward, and I am only showing
it as an example of proof by induction:

\begin{proof}
[Proof of Proposition \ref{prop.inverses.A1Ak}.]We prove Proposition
\ref{prop.inverses.A1Ak} by induction on $k$:

\textit{Induction base:} If $k=1$, then Proposition \ref{prop.inverses.A1Ak}
says that $A_{1}^{-1}=A_{1}^{-1}$; this is obviously true. Hence, Proposition
\ref{prop.inverses.A1Ak} holds for $k=1$.\ \ \ \ \footnote{A remark for the
pedants: It is common to define the product of $0$ square matrices of size
$n\times n$ (an \textquotedblleft empty product of $n\times n$%
-matrices\textquotedblright) as the identity matrix $I_{n}$ (similarly to how
a product of $0$ numbers is defined to be $1$). With this convention,
Proposition \ref{prop.inverses.A1Ak} holds for $k=0$ too (it then states that
$\left(  I_{n}\right)  ^{-1}=I_{n}$), as long as we agree what size our
non-existing matrices are considered to have (it has to be $n\times n$ for
some $n\in\mathbb{N}$). So we could have started our induction at $k=0$
instead of $k=1$.}

\textit{Induction step:} Let $\ell$ be a positive integer. Assume (as our
\textit{induction hypothesis}) that Proposition \ref{prop.inverses.A1Ak} holds
for $k=\ell$. In other words, for any $\ell$ invertible matrices $A_{1}%
,A_{2},\ldots,A_{\ell}$ for which the product $A_{1}A_{2}\cdots A_{\ell}$ is
well-defined, the matrix $A_{1}A_{2}\cdots A_{\ell}$ is invertible, and its
inverse is%
\[
\left(  A_{1}A_{2}\cdots A_{\ell}\right)  ^{-1}=A_{\ell}^{-1}A_{\ell-1}%
^{-1}\cdots A_{1}^{-1}.
\]


We must now show that Proposition \ref{prop.inverses.A1Ak} also holds for
$k=\ell+1$. So let us fix $\ell+1$ invertible matrices $A_{1},A_{2}%
,\ldots,A_{\ell+1}$ for which the product $A_{1}A_{2}\cdots A_{\ell+1}$ is
well-defined. We must then show that the matrix $A_{1}A_{2}\cdots A_{\ell+1}$
is invertible, and that its inverse is
\[
\left(  A_{1}A_{2}\cdots A_{\ell+1}\right)  ^{-1}=A_{\ell+1}^{-1}A_{\ell}%
^{-1}\cdots A_{1}^{-1}.
\]


The product $A_{1}A_{2}\cdots A_{\ell}$ is well-defined (since the product
$A_{1}A_{2}\cdots A_{\ell+1}$ is well-defined). Hence, we can apply our
induction hypothesis, and conclude that the matrix $A_{1}A_{2}\cdots A_{\ell}$
is invertible, and its inverse is%
\[
\left(  A_{1}A_{2}\cdots A_{\ell}\right)  ^{-1}=A_{\ell}^{-1}A_{\ell-1}%
^{-1}\cdots A_{1}^{-1}.
\]


Now, the matrices $A_{1}A_{2}\cdots A_{\ell}$ and $A_{\ell+1}$ are invertible,
and their product \newline$\left(  A_{1}A_{2}\cdots A_{\ell}\right)
A_{\ell+1}=A_{1}A_{2}\cdots A_{\ell+1}$ is well-defined (by assumption).
Hence, Proposition \ref{prop.inverses.AB} (applied to $A=A_{1}A_{2}\cdots
A_{\ell}$ and $B=A_{\ell+1}$) shows that the matrix $\left(  A_{1}A_{2}\cdots
A_{\ell}\right)  A_{\ell+1}$ is also invertible, and its inverse is
\[
\left(  \left(  A_{1}A_{2}\cdots A_{\ell}\right)  A_{\ell+1}\right)
^{-1}=A_{\ell+1}^{-1}\left(  A_{1}A_{2}\cdots A_{\ell}\right)  ^{-1}.
\]
Since $\left(  A_{1}A_{2}\cdots A_{\ell}\right)  A_{\ell+1}=A_{1}A_{2}\cdots
A_{\ell+1}$ and \newline$A_{\ell+1}^{-1}\underbrace{\left(  A_{1}A_{2}\cdots
A_{\ell}\right)  ^{-1}}_{=A_{\ell}^{-1}A_{\ell-1}^{-1}\cdots A_{1}^{-1}%
}=A_{\ell+1}^{-1}\left(  A_{\ell}^{-1}A_{\ell-1}^{-1}\cdots A_{1}^{-1}\right)
=A_{\ell+1}^{-1}A_{\ell}^{-1}\cdots A_{1}^{-1}$, this rewrites as follows: The
matrix matrix $A_{1}A_{2}\cdots A_{\ell+1}$ is invertible, and its inverse is
\[
\left(  A_{1}A_{2}\cdots A_{\ell+1}\right)  ^{-1}=A_{\ell+1}^{-1}A_{\ell}%
^{-1}\cdots A_{1}^{-1}.
\]
This is precisely what we wanted to show! Thus, Proposition
\ref{prop.inverses.A1Ak} holds for $k=\ell+1$. This completes the induction
step. Thus, Proposition \ref{prop.inverses.A1Ak} is proven by induction.

(I have written up this proof with a lot of detail. You do not have to! If you
are used to mathematical induction, then you can easily afford omitting many
of the incantations I made above, and taking certain shortcuts -- for example,
instead of introducing a new variable $\ell$ in the induction step, you could
reuse $k$, thus stepping \textquotedblleft from $k$ to $k+1$\textquotedblright%
\ instead of \textquotedblleft from $k=\ell$ to $k=\ell+1$\textquotedblright.
You also don't need to formally state the induction hypothesis, because it is
just a copy of the claim (with $k$ replaced by $\ell$ in our case). Finally,
what we did in our proof was obvious enough that you could just say that
\textquotedblleft Proposition \ref{prop.inverses.A1Ak} follows by a
straightforward induction on $k$, where Proposition \ref{prop.inverses.AB} is
being applied in the induction step\textquotedblright, and declare the proof finished.)
\end{proof}

\begin{corollary}
\label{cor.inverses.Ak-1}Let $n\in\mathbb{N}$. Let $k\in\mathbb{N}$. Let $A$
be an invertible $n\times n$-matrix. Then, $A^{k}$ is also invertible, and its
inverse is $\left(  A^{k}\right)  ^{-1}=\left(  A^{-1}\right)  ^{k}$.
\end{corollary}

Note that Corollary \ref{cor.inverses.Ak-1} is not obvious! You cannot argue
that $\left(  A^{k}\right)  ^{-1}=\left(  A^{-1}\right)  ^{k}$ because both
sides simplify to $A^{-k}$; this argument makes no sense unless you have
defined $A^{-k}$ (and we have not defined $A^{-k}$) and proved that standard
rules of exponentiation (such as $\left(  A^{u}\right)  ^{v}=A^{uv}$) apply to matrices.

\begin{proof}
[Proof of Corollary \ref{cor.inverses.Ak-1}.]Recall that $A^{0}=I_{n}$ (by the
definition of $A^{0}$). Hence, in the case when $k=0$, Corollary
\ref{cor.inverses.Ak-1} says that $I_{n}$ is invertible, and its inverse is
$\left(  I_{n}\right)  ^{-1}=I_{n}$. This follows from Proposition
\ref{prop.inverses.In}. Thus, Corollary \ref{cor.inverses.Ak-1} is proven in
the case when $k=0$. Therefore, we can WLOG\footnote{\textquotedblleft
WLOG\textquotedblright\ is shorthand for \textquotedblleft without loss of
generality\textquotedblright. See, for example,
\href{https://en.wikipedia.org/wiki/Without_loss_of_generality}{the Wikipedia
article for \textquotedblleft WLOG\textquotedblright} (or any book on
mathematical proofs) for the meaning of this phrase.
\par
(As far as the proof of Corollary \ref{cor.inverses.Ak-1} is concerned, the
meaning of \textquotedblleft we can WLOG assume that $k\neq0$%
\textquotedblright\ is the following: \textquotedblleft If we can prove
Corollary \ref{cor.inverses.Ak-1} for $k\neq0$, then we know how to obtain a
proof of Corollary \ref{cor.inverses.Ak-1} for all $k$ (because Corollary
\ref{cor.inverses.Ak-1} is already proven in the case when $k=0$). Thus, it
will suffice to prove Corollary \ref{cor.inverses.Ak-1} for $k\neq0$; hence,
let us assume that $k\neq0$.\textquotedblright)} assume that $k\neq0$. Assume
this. Thus, $k$ is a positive integer. Hence, $A^{k}=\underbrace{AA\cdots
A}_{k\text{ times}}$ and $\left(  A^{-1}\right)  ^{k}=\underbrace{A^{-1}%
A^{-1}\cdots A^{-1}}_{k\text{ times}}$.

Proposition \ref{prop.inverses.A1Ak} (applied to $A,A,\ldots,A$ instead of
$A_{1},A_{2},\ldots,A_{k}$) shows that the matrix $\underbrace{AA\cdots
A}_{k\text{ times}}$ is invertible, and its inverse is%
\[
\left(  \underbrace{AA\cdots A}_{k\text{ times}}\right)  ^{-1}%
=\underbrace{A^{-1}A^{-1}\cdots A^{-1}}_{k\text{ times}}.
\]
In other words, the matrix $A^{k}$ is invertible, and its inverse is $\left(
A^{k}\right)  ^{-1}=\left(  A^{-1}\right)  ^{k}$. This proves Corollary
\ref{cor.inverses.Ak-1}.
\end{proof}

\begin{proposition}
\label{prop.inverses.A-1-1}Let $n\in\mathbb{N}$. Let $A$ be an invertible
$n\times n$-matrix. Then, its inverse $A^{-1}$ is also invertible, and has the
inverse $\left(  A^{-1}\right)  ^{-1}=A$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.inverses.A-1-1}.]Since $A^{-1}$ is an inverse
of $A$, we have the two equalities $A^{-1}A=I_{n}$ and $AA^{-1}=I_{n}$. But
these very same equalities show that $A$ is an inverse of $A^{-1}$ (if you do
not trust me, just check with the definition of \textquotedblleft
inverse\textquotedblright). Thus, the matrix $A^{-1}$ is invertible, and its
inverse is $\left(  A^{-1}\right)  ^{-1}=A$. Proposition
\ref{prop.inverses.A-1-1} is proven.
\end{proof}

\begin{proposition}
\label{prop.inverses.lA}Let $n\in\mathbb{N}$. Let $\lambda$ be a nonzero
number. Let $A$ be an invertible $n\times n$-matrix. Then, the matrix $\lambda
A$ is also invertible, and its inverse is $\left(  \lambda A\right)
^{-1}=\lambda^{-1}A^{-1}=\dfrac{1}{\lambda}A^{-1}$.
\end{proposition}

\begin{exercise}
\label{exe.prop.inverses.lA}Prove Proposition \ref{prop.inverses.lA}.
\end{exercise}

\subsection{More on transposes}

How do the matrix operations we have seen above (addition, multiplication,
inversion, etc.) behave with respect to transposes? The answer is
\textquotedblleft fairly nicely\textquotedblright:

\begin{proposition}
\label{prop.transpose.opers}\textbf{(a)} Let $n\in\mathbb{N}$. Then, $\left(
I_{n}\right)  ^{T}=I_{n}$.

\textbf{(b)} Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Then, $\left(
0_{n\times m}\right)  ^{T}=0_{m\times n}$.

\textbf{(c)} Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ be an $n\times
m$-matrix. Let $\lambda$ be a number. Then, $\left(  \lambda A\right)
^{T}=\lambda A^{T}$.

\textbf{(d)} Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ and $B$ be two
$n\times m$-matrices. Then, $\left(  A+B\right)  ^{T}=A^{T}+B^{T}$.

\textbf{(e)} Let $n\in\mathbb{N}$, $m\in\mathbb{N}$ and $p\in\mathbb{N}$. Let
$A$ be an $n\times m$-matrix. Let $B$ be an $m\times p$-matrix. Then, $\left(
AB\right)  ^{T}=B^{T}A^{T}$.

\textbf{(f)} Let $n\in\mathbb{N}$. Let $A$ be an invertible $n\times
n$-matrix. Then, $A^{T}$ is invertible, and its inverse is $\left(
A^{T}\right)  ^{-1}=\left(  A^{-1}\right)  ^{T}$.
\end{proposition}

Notice that the right hand side in Proposition \ref{prop.transpose.opers}
\textbf{(e)} is $B^{T}A^{T}$, not $A^{T}B^{T}$ (in fact, $A^{T}B^{T}$ does not
always make sense, since the number of columns of $A^{T}$ is not necessarily
the number of rows of $B^{T}$). This is similar to the $B^{-1}A^{-1}$ in
Proposition \ref{prop.inverses.AB}.

Proposition \ref{prop.transpose.opers} is fairly easy to show; let us only
give a proof of part \textbf{(e)}:

\begin{proof}
[Proof of Proposition \ref{prop.transpose.opers} \textbf{(e)}.]The definition
of $A^{T}$ shows that $A^{T}=\left(  A_{j,i}\right)  _{1\leq i\leq m,\ 1\leq
j\leq n}$. Thus,%
\begin{equation}
\left(  A^{T}\right)  _{i,j}=A_{j,i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,\ldots,m\right\}  \text{ and }j\in\left\{  1,2,\ldots
,n\right\}  . \label{pf.prop.transpose.opers.d.AT}%
\end{equation}
Similarly, the definition of $B^{T}$ shows that $B^{T}=\left(  B_{j,i}\right)
_{1\leq i\leq p,\ 1\leq j\leq m}$. Hence,%
\begin{equation}
\left(  B^{T}\right)  _{i,j}=B_{j,i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,\ldots,p\right\}  \text{ and }j\in\left\{  1,2,\ldots
,m\right\}  . \label{pf.prop.transpose.opers.d.BT}%
\end{equation}


Now, $B^{T}$ is a $p\times m$-matrix, while $A^{T}$ is an $m\times n$-matrix.
Hence, $B^{T}A^{T}$ is a $p\times n$-matrix. Also, $\left(  AB\right)  ^{T}$
is a $p\times n$-matrix (since $AB$ is an $n\times p$-matrix). The definition
of $\left(  AB\right)  ^{T}$ shows that $\left(  AB\right)  ^{T}=\left(
\left(  AB\right)  _{j,i}\right)  _{1\leq i\leq p,\ 1\leq j\leq n}$. Hence,
\begin{align}
\left(  \left(  AB\right)  ^{T}\right)  _{i,j}  &  =\left(  AB\right)
_{j,i}\nonumber\\
&  =A_{j,1}B_{1,i}+A_{j,2}B_{2,i}+\cdots+A_{j,m}B_{m,i}%
\label{pf.prop.transpose.opers.d.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.matrix-prod.rc}
\textbf{(a)}, applied to }j\text{ and }i\text{ instead of }i\text{ and
}j\right) \nonumber
\end{align}
for all $i\in\left\{  1,2,\ldots,p\right\}  $ and $j\in\left\{  1,2,\ldots
,n\right\}  $.

On the other hand, we can apply Proposition \ref{prop.matrix-prod.rc}
\textbf{(a)} to $p$, $m$, $n$, $B^{T}$ and $A^{T}$ instead of $n$, $m$, $p$,
$A$ and $B$. We thus conclude that%
\begin{equation}
\left(  B^{T}A^{T}\right)  _{i,j}=\left(  B^{T}\right)  _{i,1}\left(
A^{T}\right)  _{1,j}+\left(  B^{T}\right)  _{i,2}\left(  A^{T}\right)
_{2,j}+\cdots+\left(  B^{T}\right)  _{i,m}\left(  A^{T}\right)  _{m,j}
\label{pf.prop.transpose.opers.d.2}%
\end{equation}
for all $i\in\left\{  1,2,\ldots,p\right\}  $ and $j\in\left\{  1,2,\ldots
,n\right\}  $.

But for every $k\in\left\{  1,2,\ldots,m\right\}  $, we have%
\begin{equation}
\underbrace{\left(  B^{T}\right)  _{i,k}}_{\substack{=B_{k,i}\\\text{(by
(\ref{pf.prop.transpose.opers.d.BT}), applied}\\\text{to }k\text{ instead of
}j\text{)}}}\underbrace{\left(  A^{T}\right)  _{k,j}}_{\substack{=A_{j,k}%
\\\text{(by (\ref{pf.prop.transpose.opers.d.AT}), applied}\\\text{to }k\text{
instead of }i\text{)}}}=B_{k,i}A_{j,k}=A_{j,k}B_{k,i}.
\label{pf.prop.transpose.opers.d.3}%
\end{equation}


Hence, for all $i\in\left\{  1,2,\ldots,p\right\}  $ and $j\in\left\{
1,2,\ldots,n\right\}  $, we have
\begin{align*}
\left(  B^{T}A^{T}\right)  _{i,j}  &  =\underbrace{\left(  B^{T}\right)
_{i,1}\left(  A^{T}\right)  _{1,j}}_{\substack{=A_{j,1}B_{1,i}\\\text{(by
(\ref{pf.prop.transpose.opers.d.3}), applied}\\\text{to }k=1\text{)}%
}}+\underbrace{\left(  B^{T}\right)  _{i,2}\left(  A^{T}\right)  _{2,j}%
}_{\substack{=A_{j,2}B_{2,i}\\\text{(by (\ref{pf.prop.transpose.opers.d.3}),
applied}\\\text{to }k=2\text{)}}}+\cdots+\underbrace{\left(  B^{T}\right)
_{i,m}\left(  A^{T}\right)  _{m,j}}_{\substack{=A_{j,m}B_{m,i}\\\text{(by
(\ref{pf.prop.transpose.opers.d.3}), applied}\\\text{to }k=m\text{)}%
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.transpose.opers.d.2}%
)}\right) \\
&  =A_{j,1}B_{1,i}+A_{j,2}B_{2,i}+\cdots+A_{j,m}B_{m,i}.
\end{align*}
Comparing this with (\ref{pf.prop.transpose.opers.d.1}), we conclude that
$\left(  \left(  AB\right)  ^{T}\right)  _{i,j}=\left(  B^{T}A^{T}\right)
_{i,j}$ for all $i\in\left\{  1,2,\ldots,p\right\}  $ and $j\in\left\{
1,2,\ldots,n\right\}  $. In other words, each entry of the matrix $\left(
AB\right)  ^{T}$ equals the corresponding entry of the matrix $B^{T}A^{T}$.
Thus, $\left(  AB\right)  ^{T}=B^{T}A^{T}$. This proves Proposition
\ref{prop.transpose.opers} \textbf{(e)}.
\end{proof}

\begin{exercise}
\label{exe.prop.transpose.opers.e}Prove Proposition \ref{prop.transpose.opers}
\textbf{(f)}.
\end{exercise}

\subsection{Triangular matrices}

We next discuss some particular classes of matrices: the so-called
\textit{triangular matrices} and some of their variations.

\begin{definition}
\label{def.triangular}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.

\textbf{(a)} We say that the matrix $A$ is \textit{upper-triangular} if and
only if we have%
\[
A_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }i>j.
\]


\textbf{(b)} We say that the matrix $A$ is \textit{lower-triangular} if and
only if we have%
\[
A_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }i<j.
\]


\textbf{(c)} We say that the matrix $A$ is \textit{diagonal} if and only if we
have%
\[
A_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }i\neq j.
\]

\end{definition}

Notice that only square matrices can be upper-triangular or lower-triangular
(by definition). Why the name \textquotedblleft triangular\textquotedblright?
Because visually speaking, a matrix is upper-triangular if and only if all its
entries (strictly) below the diagonal are $0$ (which means that its nonzero
entries are concentrated in the \textbf{triangle} bordered by the diagonal,
the upper rim and the right rim). I hope Example \ref{exam.triangular} will
clarify this if it is unclear. Similarly, a matrix is lower-triangular if and
only if all its entries (strictly) above the diagonal are $0$ (which again
means that its nonzero entries are concentrated in a triangle, this time to
the southwest of the diagonal). Finally, a matrix is diagonal if only if all
its entries except for the diagonal entries are $0$.

I think the following example should explain this:

\begin{example}
\label{exam.triangular}\textbf{(a)} A $4\times4$-matrix is upper-triangular if
and only if it has the form $\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
0 & b^{\prime} & c^{\prime} & d^{\prime}\\
0 & 0 & c^{\prime\prime} & d^{\prime\prime}\\
0 & 0 & 0 & d^{\prime\prime\prime}%
\end{array}
\right)  $ for some numbers $a,b,c,d,b^{\prime},c^{\prime},d^{\prime
},c^{\prime\prime},d^{\prime\prime},d^{\prime\prime\prime}$. Notice that we
are making no requirements on these numbers; in particular, they \textbf{can}
be $0$. Upper-triangularity means that $A_{i,j}=0$ whenever $i>j$; it does
\textbf{not} require that $A_{i,j}\neq0$ in all other cases.

\textbf{(b)} A $4\times4$-matrix is lower-triangular if and only if it has the
form $\left(
\begin{array}
[c]{cccc}%
a & 0 & 0 & 0\\
a^{\prime} & b^{\prime} & 0 & 0\\
a^{\prime\prime} & b^{\prime\prime} & c^{\prime\prime} & 0\\
a^{\prime\prime\prime} & b^{\prime\prime\prime} & c^{\prime\prime\prime} &
d^{\prime\prime}%
\end{array}
\right)  $ for some numbers $a,a^{\prime},b^{\prime},a^{\prime\prime
},b^{\prime\prime},c^{\prime\prime},a^{\prime\prime\prime},b^{\prime
\prime\prime},c^{\prime\prime\prime},d^{\prime\prime\prime}$.

\textbf{(c)} A $4\times4$-matrix is diagonal if and only if it has the form
$\left(
\begin{array}
[c]{cccc}%
a & 0 & 0 & 0\\
0 & b^{\prime} & 0 & 0\\
0 & 0 & c^{\prime\prime} & 0\\
0 & 0 & 0 & d^{\prime\prime\prime}%
\end{array}
\right)  $ for some numbers $a,b^{\prime},c^{\prime\prime},d^{\prime
\prime\prime}$.
\end{example}

Here is something obvious:

\begin{proposition}
\label{prop.triangular.obvious}Let $n\in\mathbb{N}$.

\textbf{(a)} An $n\times n$-matrix $A$ is diagonal if and only if $A$ is both
upper-triangular and lower-triangular.

\textbf{(b)} The zero matrix $0_{n\times n}$ and the identity matrix $I_{n}$
are upper-triangular, lower-triangular and diagonal.
\end{proposition}

A less trivial fact is that the product of two upper-triangular matrices is
upper-triangular again. We shall show this, and a little bit more, in the
following theorem:

\begin{theorem}
\label{thm.triangular.prod-up}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two
upper-triangular $n\times n$-matrices.

\textbf{(a)} Then, $AB$ is an upper-triangular $n\times n$-matrix.

\textbf{(b)} The diagonal entries of $AB$ are%
\[
\left(  AB\right)  _{i,i}=A_{i,i}B_{i,i}\ \ \ \ \ \ \ \ \ \ \text{for all
}i\in\left\{  1,2,\ldots,n\right\}  .
\]


\textbf{(c)} Also, $A+B$ is an upper-triangular $n\times n$-matrix.
Furthermore, $\lambda A$ is an upper-triangular matrix whenever $\lambda$ is a number.
\end{theorem}

Note that Theorem \ref{thm.triangular.prod-up} \textbf{(b)} says that each
diagonal entry of $AB$ is the product of the corresponding diagonal entries of
$A$ and of $B$. Thus, \textbf{in this specific case}, the product $AB$ does
behave as if matrices were multiplied entry by entry (but only for its
diagonal entries). Before I prove Theorem \ref{thm.triangular.prod-up}, let me
give an example:

\begin{example}
Let $A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
0 & b^{\prime} & c^{\prime}\\
0 & 0 & c^{\prime\prime}%
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{ccc}%
x & y & z\\
0 & y^{\prime} & z^{\prime}\\
0 & 0 & z^{\prime\prime}%
\end{array}
\right)  $ be two upper-triangular $3\times3$-matrices. Then,%
\[
AB=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
0 & b^{\prime} & c^{\prime}\\
0 & 0 & c^{\prime\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
x & y & z\\
0 & y^{\prime} & z^{\prime}\\
0 & 0 & z^{\prime\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
ax & ay+by^{\prime} & az+bz^{\prime}+cz^{\prime\prime}\\
0 & b^{\prime}y^{\prime} & b^{\prime}z^{\prime}+c^{\prime}z^{\prime\prime}\\
0 & 0 & c^{\prime\prime}z^{\prime\prime}%
\end{array}
\right)  .
\]
Thus, $AB$ is again upper-triangular (as Theorem \ref{thm.triangular.prod-up}
\textbf{(a)} predicts), and the diagonal entries $ax,b^{\prime}y^{\prime
},c^{\prime\prime}z^{\prime\prime}$ of $AB$ are the products of the respective
entries of $A$ and of $B$ (as Theorem \ref{thm.triangular.prod-up}
\textbf{(b)} predicts).
\end{example}

\begin{proof}
[Proof of Theorem \ref{thm.triangular.prod-up}.]The matrix $A$ is
upper-triangular. In other words,%
\begin{equation}
A_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }i>j
\label{pf.thm.triangular.prod-up.A-tri}%
\end{equation}
(because this is what it means for $A$ to be upper-triangular). Similarly,%
\begin{equation}
B_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }i>j
\label{pf.thm.triangular.prod-up.B-tri}%
\end{equation}
(because $B$, too, is upper-triangular).

Now, fix two elements $i$ and $j$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $i>j$. We shall prove that for every $k\in\left\{  1,2,\ldots
,n\right\}  $, we have
\begin{equation}
A_{i,k}B_{k,j}=0. \label{pf.thm.triangular.prod-up.AB0}%
\end{equation}


[\textit{Proof of (\ref{pf.thm.triangular.prod-up.AB0}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. Then, we are in one of the following two cases:

\textit{Case 1:} We have $i\leq k$.

\textit{Case 2:} We have $i>k$.

We shall prove (\ref{pf.thm.triangular.prod-up.AB0}) in each of these two
cases separately:

\begin{enumerate}
\item Let us first consider Case 1. In this case, we have $i\leq k$. Thus,
$k\geq i$, so that $k\geq i>j$. Hence, we can apply
(\ref{pf.thm.triangular.prod-up.B-tri}) to $k$ instead of $i$. As a result, we
obtain $B_{k,j}=0$. Hence, $A_{i,k}\underbrace{B_{k,j}}_{=0}=A_{i,k}0=0$.
Thus, (\ref{pf.thm.triangular.prod-up.AB0}) is proven in Case 1.

\item Let us now consider Case 2. In this case, we have $i>k$. Hence, we can
apply (\ref{pf.thm.triangular.prod-up.A-tri}) to $k$ instead of $j$. As a
result, we obtain $A_{i,k}=0$. Hence, $\underbrace{A_{i,k}}_{=0}%
B_{k,j}=0B_{k,j}=0$. Thus, (\ref{pf.thm.triangular.prod-up.AB0}) is proven in
Case 2.
\end{enumerate}

We have now proven (\ref{pf.thm.triangular.prod-up.AB0}) in both Cases 1 and
2. Thus, (\ref{pf.thm.triangular.prod-up.AB0}) is proven.]

Now, Proposition \ref{prop.matrix-prod.rc} \textbf{(a)} shows that%
\begin{align*}
\left(  AB\right)  _{i,j}  &  =\underbrace{A_{i,1}B_{1,j}}%
_{\substack{=0\\\text{(by (\ref{pf.thm.triangular.prod-up.AB0}),}%
\\\text{applied to }k=1\text{)}}}+\underbrace{A_{i,2}B_{2,j}}%
_{\substack{=0\\\text{(by (\ref{pf.thm.triangular.prod-up.AB0}),}%
\\\text{applied to }k=2\text{)}}}+\cdots+\underbrace{A_{i,m}B_{m,j}%
}_{\substack{=0\\\text{(by (\ref{pf.thm.triangular.prod-up.AB0}),}%
\\\text{applied to }k=m\text{)}}}\\
&  =0+0+\cdots+0=0.
\end{align*}


Now, forget that we fixed $i$ and $j$. We thus have shown that%
\begin{equation}
\left(  AB\right)  _{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }i>j.
\label{pf.thm.triangular.prod-up.AB0a}%
\end{equation}
But this says precisely that $AB$ is upper-triangular. Thus, Theorem
\ref{thm.triangular.prod-up} \textbf{(a)} is proven.

\textbf{(b)} Let $i\in\left\{  1,2,\ldots,n\right\}  $. We must prove that
$\left(  AB\right)  _{i,i}=A_{i,i}B_{i,i}$.

In a sense, this is similar to how we proved
(\ref{pf.thm.triangular.prod-up.AB0a}), but a little bit more complicated.

We first observe that, for every $k\in\left\{  1,2,\ldots,n\right\}  $
satisfying $k\neq i$, we have%
\begin{equation}
A_{i,k}B_{k,i}=0. \label{pf.thm.triangular.prod-up.AB1}%
\end{equation}


The proof of this will be very similar to the proof of
(\ref{pf.thm.triangular.prod-up.AB0}), with $j$ replaced by $i$:

[\textit{Proof of (\ref{pf.thm.triangular.prod-up.AB1}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $ be such that $k\neq i$. Then, we are in one of the
following two cases:

\textit{Case 1:} We have $i\leq k$.

\textit{Case 2:} We have $i>k$.

We shall prove (\ref{pf.thm.triangular.prod-up.AB1}) in each of these two
cases separately:

\begin{enumerate}
\item Let us first consider Case 1. In this case, we have $i\leq k$. Thus,
$k\geq i$, so that $k>i$ (because $k\neq i$). Hence, we can apply
(\ref{pf.thm.triangular.prod-up.B-tri}) to $k$ and $i$ instead of $i$ and $j$.
As a result, we obtain $B_{k,i}=0$. Hence, $A_{i,k}\underbrace{B_{k,i}}%
_{=0}=A_{i,k}0=0$. Thus, (\ref{pf.thm.triangular.prod-up.AB1}) is proven in
Case 1.

\item Let us now consider Case 2. In this case, we have $i>k$. Hence, we can
apply (\ref{pf.thm.triangular.prod-up.A-tri}) to $k$ instead of $j$. As a
result, we obtain $A_{i,k}=0$. Hence, $\underbrace{A_{i,k}}_{=0}%
B_{k,i}=0B_{k,i}=0$. Thus, (\ref{pf.thm.triangular.prod-up.AB1}) is proven in
Case 2.
\end{enumerate}

We have now proven (\ref{pf.thm.triangular.prod-up.AB1}) in both Cases 1 and
2. Thus, (\ref{pf.thm.triangular.prod-up.AB1}) is proven.]

Now, Proposition \ref{prop.matrix-prod.rc} \textbf{(a)} (applied to $j=i$)
shows that%
\begin{align*}
&  \left(  AB\right)  _{i,i}\\
&  =A_{i,1}B_{1,i}+A_{i,2}B_{2,i}+\cdots+A_{i,m}B_{m,i}\\
&  =\left(  \text{the sum of the terms }A_{i,k}B_{k,i}\text{ for all }%
k\in\left\{  1,2,\ldots,n\right\}  \right) \\
&  =A_{i,i}B_{i,i}+\underbrace{\left(  \text{the sum of the terms }%
A_{i,k}B_{k,i}\text{ for all }k\in\left\{  1,2,\ldots,n\right\}  \text{
satisfying }k\neq i\right)  }_{\substack{=0\\\text{(because
(\ref{pf.thm.triangular.prod-up.AB1}) shows that all of these terms are
}0\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have taken the term }%
A_{i,i}B_{i,i}\text{ out of the sum}\right) \\
&  =A_{i,i}B_{i,i}+0=A_{i,i}B_{i,i}.
\end{align*}
This proves Theorem \ref{thm.triangular.prod-up} \textbf{(b)}.

\textbf{(c)} Theorem \ref{thm.triangular.prod-up} \textbf{(c)} is
straightforward to check (due to the simple definitions of $A+B$ and $\lambda
A$); the details are left to the reader.
\end{proof}

The natural analogue of Theorem \ref{thm.triangular.prod-up} for
lower-triangular matrices also holds:

\begin{theorem}
\label{thm.triangular.prod-down}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two
lower-triangular $n\times n$-matrices.

\textbf{(a)} Then, $AB$ is a lower-triangular $n\times n$-matrix.

\textbf{(b)} The diagonal entries of $AB$ are%
\[
\left(  AB\right)  _{i,i}=A_{i,i}B_{i,i}\ \ \ \ \ \ \ \ \ \ \text{for all
}i\in\left\{  1,2,\ldots,n\right\}  .
\]


\textbf{(c)} Also, $A+B$ is a lower-triangular $n\times n$-matrix.
Furthermore, $\lambda A$ is a lower-triangular matrix whenever $\lambda$ is a number.
\end{theorem}

The proof of Theorem \ref{thm.triangular.prod-down} is analogous to that of
Theorem \ref{thm.triangular.prod-up}, and the changes required are fairly
straightforward (change some inequality signs). Let me pose this as an exercise:

\begin{exercise}
\label{exe.thm.triangular.prod-down}Prove Theorem
\ref{thm.triangular.prod-down} \textbf{(a)}. (Feel free to repeat my proof of
Theorem \ref{thm.triangular.prod-up} \textbf{(a)}, changing only what little
needs to be changed. This is not plagiarism for the purpose of this exercise!)

(Similarly, you can prove Theorem \ref{thm.triangular.prod-down} \textbf{(b)}
and \textbf{(c)}, but you don't need to write it up.)
\end{exercise}

Lower-triangular and upper-triangular matrices are not only analogues of each
other; they are also closely related:

\begin{proposition}
\label{prop.triangular.UT=D}Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix. Then, $A$ is upper-triangular if and only if $A^{T}$ is lower-triangular.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.triangular.UT=D}.]The definition of $A^{T}$
shows that $A^{T}=\left(  A_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Thus,%
\begin{equation}
\left(  A^{T}\right)  _{i,j}=A_{j,i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,\ldots,n\right\}  \text{ and }j\in\left\{  1,2,\ldots
,n\right\}  . \label{pf.prop.triangular.UT=D.1}%
\end{equation}


Now, consider the following chain of equivalent statements\footnote{After each
equivalence, we give a justification for why it is an equivalence.}:%
\begin{align*}
&  \ \left(  A\text{ is upper-triangular}\right) \\
&  \Longleftrightarrow\ \left(  A_{i,j}=0\text{ whenever }i>j\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{because this is how \textquotedblleft
upper-triangular\textquotedblright\ is defined}\right) \\
&  \Longleftrightarrow\ \left(  A_{j,i}=0\text{ whenever }j>i\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have just renamed }i\text{ and
}j\text{ as }j\text{ and }i\right) \\
&  \Longleftrightarrow\ \left(  A_{j,i}=0\text{ whenever }i<j\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{because }j>i\text{ is equivalent to
}i<j\right) \\
&  \Longleftrightarrow\ \left(  \left(  A^{T}\right)  _{i,j}=0\text{ whenever
}i<j\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have replaced }A_{j,i}\text{ by
}\left(  A^{T}\right)  _{i,j}\text{, because of
(\ref{pf.prop.triangular.UT=D.1})}\right) \\
&  \Longleftrightarrow\ \left(  A^{T}\text{ is lower-triangular}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{because this is how \textquotedblleft
lower-triangular\textquotedblright\ is defined}\right)  .
\end{align*}
Thus, Proposition \ref{prop.triangular.UT=D} holds.
\end{proof}

There are a few special classes of triangular matrices worth giving names:

\begin{definition}
\label{def.triangular.uni-up}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.

\textbf{(a)} The matrix $A$ is said to be \textit{upper-unitriangular} if and
only if it is upper-triangular and all its diagonal entries are $1$ (that is,
$A_{i,i}=1$ for all $i$).

\textbf{(b)} The matrix $A$ is said to be \textit{invertibly upper-triangular}
if and only if it is upper-triangular and all its diagonal entries are nonzero
(that is, $A_{i,i}\neq0$ for all $i$).

\textbf{(c)} The matrix $A$ is said to be \textit{strictly upper-triangular}
if and only if it is upper-triangular and all its diagonal entries are $0$
(that is, $A_{i,i}=0$ for all $i$).

Similar notions can be defined with \textquotedblleft lower\textquotedblright%
\ instead of \textquotedblleft upper\textquotedblright.
\end{definition}

The words we have just defined are not as important as the word
\textquotedblleft upper-triangular\textquotedblright\ (you certainly don't
need to learn them by heart); but these notions appear from time to time in
mathematics, and it helps if you know how to recognize them.

\begin{example}
\textbf{(a)} A $3\times3$-matrix is upper-unitriangular if and only if it has
the form $\left(
\begin{array}
[c]{ccc}%
1 & b & c\\
0 & 1 & c^{\prime}\\
0 & 0 & 1
\end{array}
\right)  $ for some $b,c,c^{\prime}$.

\textbf{(b)} A $3\times3$-matrix is invertibly upper-triangular if and only if
it has the form $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
0 & b^{\prime} & c^{\prime}\\
0 & 0 & c^{\prime\prime}%
\end{array}
\right)  $ for some $a,b,c,b^{\prime},c^{\prime},c^{\prime\prime}$ with
$a\neq0$, $b^{\prime}\neq0$ and $c^{\prime\prime}\neq0$.

\textbf{(c)} A $3\times3$-matrix is strictly upper-triangular if and only if
it has the form $\left(
\begin{array}
[c]{ccc}%
0 & b & c\\
0 & 0 & c^{\prime}\\
0 & 0 & 0
\end{array}
\right)  $ for some $b,c,c^{\prime}$.
\end{example}

Olver and Shakiban (in \cite{OlvSha06}) use the word \textquotedblleft special
upper triangular\textquotedblright\ instead of \textquotedblleft
upper-unitriangular\textquotedblright. But I prefer \textquotedblleft
upper-unitriangular\textquotedblright, since the word \textquotedblleft
uni\textquotedblright\ hints directly to the definition (namely, the $1$'s on
the diagonal), whereas the word \textquotedblleft special\textquotedblright%
\ can mean pretty much anything.

The word \textquotedblleft invertibly upper-triangular\textquotedblright\ is
my invention. We will later see it vindicated, once we show that an
upper-triangular matrix is invertible if and only if it is invertibly upper-triangular.

Clearly, upper-unitriangular matrices are invertibly upper-triangular (since
$1$ is nonzero).

Strictly upper-triangular $n\times n$-matrices can also be characterized as
the $n\times n$-matrices $A$ which satisfy%
\[
A_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }i\geq j.
\]
Notice the weak inequality \textquotedblleft$i\geq j$\textquotedblright\ (as
opposed to the strict inequality \textquotedblleft$i>j$\textquotedblright\ in
the definition of upper-triangular matrices).

\begin{exercise}
\label{exe.inverses.3x3-ut}Let $A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
0 & b^{\prime} & c^{\prime}\\
0 & 0 & c^{\prime\prime}%
\end{array}
\right)  $ be an invertibly upper-triangular $3\times3$-matrix. Show that $A$
is invertible by explicitly computing the inverse of $A$ (in terms of
$a,b,c,b^{\prime},c^{\prime},c^{\prime\prime}$).

[\textbf{Hint:} In order to find a right inverse of $A$, it is enough to find
three column vectors $u,v,w$ (each of size $3$) satisfying the equations%
\[
Au=\left(
\begin{array}
[c]{c}%
1\\
0\\
0
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ Av=\left(
\begin{array}
[c]{c}%
0\\
1\\
0
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ Aw=\left(
\begin{array}
[c]{c}%
0\\
0\\
1
\end{array}
\right)  .
\]
In fact, once these vectors are found, assembling them into a matrix yields a
right inverse of $A$ (why?). Find these $u,v,w$. Then, check that the
resulting right-inverse of $A$ is also a left-inverse.]
\end{exercise}

\begin{corollary}
\label{cor.triangular.uni*uni}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two
upper-unitriangular $n\times n$-matrices. Then, $AB$ is also an
upper-unitriangular $n\times n$-matrix.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.triangular.uni*uni}.]The matrices $A$ and $B$ are
upper-triangular (since they are upper-unitriangular). Hence, Theorem
\ref{thm.triangular.prod-up} \textbf{(a)} shows that $AB$ is an
upper-triangular $n\times n$-matrix. Moreover, Theorem
\ref{thm.triangular.prod-up} \textbf{(a)} shows that
\[
\left(  AB\right)  _{i,i}=\underbrace{A_{i,i}}_{\substack{=1\\\text{(since
}A\text{ is}\\\text{unitriangular)}}}\underbrace{B_{i,i}}%
_{\substack{=1\\\text{(since }B\text{ is}\\\text{unitriangular)}}}=1\cdot1=1
\]
for all $i\in\left\{  1,2,\ldots,n\right\}  $. Thus, the matrix $AB$ is
upper-unitriangular (since we already know that $AB$ is upper-triangular).
Corollary \ref{cor.triangular.uni*uni} is thus proven.
\end{proof}

[to be continued]

\begin{thebibliography}{999999999}                                                                                        %


\bibitem[deBoor]{deBoor}Carl de Boor, \textit{An empty exercise}.
\url{ftp://ftp.cs.wisc.edu/Approx/empty.pdf} .

\bibitem[Drucker12]{Drucker12}D. Drucker, \textit{Annotated Bibliography of
Linear Algebra Books}, February 2012.\newline\url{http://www.math.wayne.edu/~drucker/linalgrefsW12.pdf}

\bibitem[OlvSha06]{OlvSha06}Peter J. Olver, Chehrzad Shakiban, \textit{Applied
Linear Algebra}, Prentice Hall, 2006.

\bibitem[Heffer16]{Heffer16}Jim Hefferon, \textit{Linear Algebra}, 2016.
(Scroll down to \textquotedblleft The next edition\textquotedblright\ on
\url{http://joshua.smcvt.edu/linearalgebra/} .)

\bibitem[LaNaSc16]{LaNaSc16}Isaiah Lankham, Bruno Nachtergaele, Anne
Schilling, \textit{Linear Algebra As an Introduction to Abstract Mathematics},
2016. (Download from
\url{https://www.math.ucdavis.edu/~anne/linear_algebra/mat67_course_notes.pdf} .)

\bibitem[Treil15]{Treil15}Sergei Treil, \textit{Linear Algebra Done Wrong},
2015. (Download from
\url{https://www.math.brown.edu/~treil/papers/LADW/book.pdf} .)

\bibitem[Zuker14]{Zuker14}M. Zuker, \textit{Binary Operations \& General
Associativity},\newline\url{http://snark.math.rpi.edu/Teaching/MATH-4010/Binary_Ops.pdf}
\end{thebibliography}


\end{document}